{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6466712-714f-44e9-bcf6-17d9a0ed9262",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install ipywidgets --quiet\n",
    "!pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f751026-8970-47d6-8307-6507d0730857",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ab5eff8-958f-44b9-952c-fd6c08597387",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "aws_role = get_execution_role()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "829b431d-d9d6-413f-845a-9613d61065d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def query(model_predictor, payload, content_type, accept):\n",
    "    \"\"\"Query the model predictor.\"\"\"\n",
    "\n",
    "    query_response = model_predictor.predict(\n",
    "        payload,\n",
    "        {\n",
    "            \"ContentType\": content_type,\n",
    "            \"Accept\": accept,\n",
    "        },\n",
    "    )\n",
    "    return query_response\n",
    "\n",
    "\n",
    "def parse_response(query_response):\n",
    "    \"\"\"Parse response and return the generated images.\"\"\"\n",
    "\n",
    "    response_dict = json.loads(query_response)\n",
    "    return response_dict[\"generated_images\"]\n",
    "\n",
    "\n",
    "def display_img_and_prompt(img, prmpt):\n",
    "    \"\"\"Display the generated image.\"\"\"\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(np.array(img))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(prmpt)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d61df0d-7ac0-4005-a03f-cccf68b2e8bf",
   "metadata": {},
   "source": [
    "# Deploy stable diffusion inpainting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ed85aac-4344-482d-8fe0-d67514442ac9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model-inpainting-runwayml-stable-diffusion-inpainting',\n",
       " 'model-inpainting-runwayml-stable-diffusion-inpainting-fp16',\n",
       " 'model-inpainting-stabilityai-stable-diffusion-2-inpainting',\n",
       " 'model-inpainting-stabilityai-stable-diffusion-2-inpainting-fp16',\n",
       " 'model-inpainting-stabilityai-stable-diffusion2-inpainting-fp16']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from ipywidgets import Dropdown\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "\n",
    "# Retrieves all Text-to-Image generation models.\n",
    "filter_value = \"task == inpainting\"\n",
    "inpainting_models = list_jumpstart_models(filter=filter_value)\n",
    "\n",
    "# display the model-ids in a dropdown to select a model for inference.\n",
    "inpainting_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f32614b-e992-44b6-851b-024952f37889",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_id, model_version = \"model-inpainting-stabilityai-stable-diffusion-2-inpainting\", \"*\"\n",
    "model_id, model_version = \"model-inpainting-runwayml-stable-diffusion-inpainting-fp16\", \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "193133bc-4d2b-4f07-97f2-d5c74a266e24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using model 'model-inpainting-runwayml-stable-diffusion-inpainting-fp16' with wildcard version identifier '*'. You can pin to version '2.0.1' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "\n",
    "endpoint_name = name_from_base(f\"jumpstart-example-{model_id}\")\n",
    "\n",
    "# Instances with more GPU memory supports generation of larger images.\n",
    "# So, please select instance types such as ml.g5.2xlarge if you want to generate a very large image.\n",
    "inference_instance_type = \"ml.g5.2xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri. This is the base HuggingFace container image for the default model above.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the model uri. This includes the pre-trained model and parameters as well as the inference scripts.\n",
    "# This includes all dependencies and scripts for model loading, inference handling etc..\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "# Create the SageMaker model instance\n",
    "model = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    model_data=model_uri,\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14b402a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploy_image_uri: \n",
      " 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-inference:1.10.2-transformers4.17.0-gpu-py38-cu113-ubuntu20.04\n",
      "model_uri: \n",
      " s3://jumpstart-cache-prod-us-west-2/stabilityai-inpainting/model-inpainting-runwayml-stable-diffusion-inpainting-fp16/artifacts/inference-prepack/v1.0.0/\n"
     ]
    }
   ],
   "source": [
    "print(\"deploy_image_uri: \\n\", deploy_image_uri)\n",
    "print(\"model_uri: \\n\", model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "648a4772-9b57-4e8b-9ba1-3b30c7a61722",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateModel operation: Could not find model data at s3://jumpstart-cache-prod-us-west-2/stabilityai-inpainting/model-inpainting-runwayml-stable-diffusion-inpainting-fp16/artifacts/inference-prepack/v1.0.0/.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/sagemaker-user/Self-Study-Generative-AI/05-Image-Inpainting/inpainting.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ngfbyyfevkn2rl2.studio.us-west-2.sagemaker.aws/home/sagemaker-user/Self-Study-Generative-AI/05-Image-Inpainting/inpainting.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# deploy the Model. Note that we need to pass Predictor class when we deploy model through Model class,\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ngfbyyfevkn2rl2.studio.us-west-2.sagemaker.aws/home/sagemaker-user/Self-Study-Generative-AI/05-Image-Inpainting/inpainting.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# for being able to run inference through the sagemaker API.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ngfbyyfevkn2rl2.studio.us-west-2.sagemaker.aws/home/sagemaker-user/Self-Study-Generative-AI/05-Image-Inpainting/inpainting.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m model_predictor \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdeploy(\n\u001b[1;32m      <a href='vscode-notebook-cell://ngfbyyfevkn2rl2.studio.us-west-2.sagemaker.aws/home/sagemaker-user/Self-Study-Generative-AI/05-Image-Inpainting/inpainting.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     initial_instance_count\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ngfbyyfevkn2rl2.studio.us-west-2.sagemaker.aws/home/sagemaker-user/Self-Study-Generative-AI/05-Image-Inpainting/inpainting.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     instance_type\u001b[39m=\u001b[39;49minference_instance_type,\n\u001b[1;32m      <a href='vscode-notebook-cell://ngfbyyfevkn2rl2.studio.us-west-2.sagemaker.aws/home/sagemaker-user/Self-Study-Generative-AI/05-Image-Inpainting/inpainting.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     predictor_cls\u001b[39m=\u001b[39;49mPredictor,\n\u001b[1;32m      <a href='vscode-notebook-cell://ngfbyyfevkn2rl2.studio.us-west-2.sagemaker.aws/home/sagemaker-user/Self-Study-Generative-AI/05-Image-Inpainting/inpainting.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     endpoint_name\u001b[39m=\u001b[39;49mendpoint_name,\n\u001b[1;32m      <a href='vscode-notebook-cell://ngfbyyfevkn2rl2.studio.us-west-2.sagemaker.aws/home/sagemaker-user/Self-Study-Generative-AI/05-Image-Inpainting/inpainting.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Self-Study-Generative-AI/.conda/lib/python3.10/site-packages/sagemaker/model.py:1698\u001b[0m, in \u001b[0;36mModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, accept_eula, endpoint_logging, resources, endpoint_type, managed_instance_scaling, inference_component_name, routing_config, model_reference_arn, **kwargs)\u001b[0m\n\u001b[1;32m   1695\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1697\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# existing single model endpoint path\u001b[39;00m\n\u001b[0;32m-> 1698\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_sagemaker_model(\n\u001b[1;32m   1699\u001b[0m         instance_type\u001b[39m=\u001b[39;49minstance_type,\n\u001b[1;32m   1700\u001b[0m         accelerator_type\u001b[39m=\u001b[39;49maccelerator_type,\n\u001b[1;32m   1701\u001b[0m         tags\u001b[39m=\u001b[39;49mtags,\n\u001b[1;32m   1702\u001b[0m         serverless_inference_config\u001b[39m=\u001b[39;49mserverless_inference_config,\n\u001b[1;32m   1703\u001b[0m         accept_eula\u001b[39m=\u001b[39;49maccept_eula,\n\u001b[1;32m   1704\u001b[0m         model_reference_arn\u001b[39m=\u001b[39;49mmodel_reference_arn,\n\u001b[1;32m   1705\u001b[0m     )\n\u001b[1;32m   1706\u001b[0m     serverless_inference_config_dict \u001b[39m=\u001b[39m (\n\u001b[1;32m   1707\u001b[0m         serverless_inference_config\u001b[39m.\u001b[39m_to_request_dict() \u001b[39mif\u001b[39;00m is_serverless \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1708\u001b[0m     )\n\u001b[1;32m   1709\u001b[0m     production_variant \u001b[39m=\u001b[39m sagemaker\u001b[39m.\u001b[39mproduction_variant(\n\u001b[1;32m   1710\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname,\n\u001b[1;32m   1711\u001b[0m         instance_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1718\u001b[0m         routing_config\u001b[39m=\u001b[39mrouting_config,\n\u001b[1;32m   1719\u001b[0m     )\n",
      "File \u001b[0;32m~/Self-Study-Generative-AI/.conda/lib/python3.10/site-packages/sagemaker/model.py:980\u001b[0m, in \u001b[0;36mModel._create_sagemaker_model\u001b[0;34m(self, instance_type, accelerator_type, tags, serverless_inference_config, accept_eula, model_reference_arn)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39m=\u001b[39m resolve_nested_dict_value_from_config(\n\u001b[1;32m    967\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv,\n\u001b[1;32m    968\u001b[0m     [\u001b[39m\"\u001b[39m\u001b[39mEnvironment\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    969\u001b[0m     MODEL_CONTAINERS_PATH,\n\u001b[1;32m    970\u001b[0m     sagemaker_session\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session,\n\u001b[1;32m    971\u001b[0m )\n\u001b[1;32m    972\u001b[0m create_model_args \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m    973\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname,\n\u001b[1;32m    974\u001b[0m     role\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrole,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    978\u001b[0m     tags\u001b[39m=\u001b[39mformat_tags(tags),\n\u001b[1;32m    979\u001b[0m )\n\u001b[0;32m--> 980\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49mcreate_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcreate_model_args)\n",
      "File \u001b[0;32m~/Self-Study-Generative-AI/.conda/lib/python3.10/site-packages/sagemaker/session.py:4040\u001b[0m, in \u001b[0;36mSession.create_model\u001b[0;34m(self, name, role, container_defs, vpc_config, enable_network_isolation, primary_container, tags)\u001b[0m\n\u001b[1;32m   4037\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4038\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[0;32m-> 4040\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_intercept_create_request(create_model_request, submit, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_model\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[1;32m   4041\u001b[0m \u001b[39mreturn\u001b[39;00m name\n",
      "File \u001b[0;32m~/Self-Study-Generative-AI/.conda/lib/python3.10/site-packages/sagemaker/session.py:6606\u001b[0m, in \u001b[0;36mSession._intercept_create_request\u001b[0;34m(self, request, create, func_name)\u001b[0m\n\u001b[1;32m   6589\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_intercept_create_request\u001b[39m(\n\u001b[1;32m   6590\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   6591\u001b[0m     request: typing\u001b[39m.\u001b[39mDict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6594\u001b[0m     \u001b[39m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[1;32m   6595\u001b[0m ):\n\u001b[1;32m   6596\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This function intercepts the create job request.\u001b[39;00m\n\u001b[1;32m   6597\u001b[0m \n\u001b[1;32m   6598\u001b[0m \u001b[39m    PipelineSession inherits this Session class and will override\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6604\u001b[0m \u001b[39m        func_name (str): the name of the function needed intercepting\u001b[39;00m\n\u001b[1;32m   6605\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 6606\u001b[0m     \u001b[39mreturn\u001b[39;00m create(request)\n",
      "File \u001b[0;32m~/Self-Study-Generative-AI/.conda/lib/python3.10/site-packages/sagemaker/session.py:4028\u001b[0m, in \u001b[0;36mSession.create_model.<locals>.submit\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m   4026\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mCreateModel request: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, json\u001b[39m.\u001b[39mdumps(request, indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m))\n\u001b[1;32m   4027\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 4028\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_client\u001b[39m.\u001b[39;49mcreate_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrequest)\n\u001b[1;32m   4029\u001b[0m \u001b[39mexcept\u001b[39;00m ClientError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   4030\u001b[0m     error_code \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39mresponse[\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Self-Study-Generative-AI/.conda/lib/python3.10/site-packages/botocore/client.py:569\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    566\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m~/Self-Study-Generative-AI/.conda/lib/python3.10/site-packages/botocore/client.py:1023\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     error_code \u001b[39m=\u001b[39m error_info\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mQueryErrorCode\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m error_info\u001b[39m.\u001b[39mget(\n\u001b[1;32m   1020\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1023\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1024\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateModel operation: Could not find model data at s3://jumpstart-cache-prod-us-west-2/stabilityai-inpainting/model-inpainting-runwayml-stable-diffusion-inpainting-fp16/artifacts/inference-prepack/v1.0.0/."
     ]
    }
   ],
   "source": [
    "# deploy the Model. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "model_predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    predictor_cls=Predictor,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2dc0ff0-29f2-4fa4-961e-fcfe347acc34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image, ImageDraw, ImageFilter\n",
    "\n",
    "rekognition = boto3.client('rekognition')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9044a2f-18b4-4c53-9e9c-2385e8404f4b",
   "metadata": {},
   "source": [
    "# masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8daecb7c-f0fc-4317-a72c-a47e21404b19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "portrait_image = \"portrait_iamge/version1-result.png\"\n",
    "with open(portrait_image, 'rb') as image_file:\n",
    "    image_bytes = image_file.read()\n",
    "    \n",
    "# 원본 이미지 열기\n",
    "image = Image.open(io.BytesIO(image_bytes))\n",
    "width, height = image.size\n",
    "\n",
    "# 마스크 생성 (초기값: 약간의 블러)\n",
    "mask = Image.new(mode=\"RGB\", size=(width, height), color=(255,255,255))  # 배경을 약간 흐리게 하기 위해 220으로 설정\n",
    "draw = ImageDraw.Draw(mask)\n",
    "\n",
    "# Rekognition으로 얼굴 감지\n",
    "response = rekognition.detect_faces(\n",
    "    Image={'Bytes': image_bytes},\n",
    "    Attributes=['DEFAULT']\n",
    ")\n",
    "\n",
    "# 각 얼굴에 대해 처리\n",
    "for face_detail in response['FaceDetails']:\n",
    "    bbox = face_detail['BoundingBox']\n",
    "\n",
    "    # 바운딩 박스 좌표 계산\n",
    "    left = int(bbox['Left'] * width)\n",
    "    top = int(bbox['Top'] * height)\n",
    "    right = int((bbox['Left'] + bbox['Width']) * width)\n",
    "    bottom = int((bbox['Top'] + bbox['Height']) * height)\n",
    "\n",
    "    # 얼굴 영역을 검은색으로 채우기\n",
    "    # 패딩을 추가하여 얼굴 주변을 더 자연스럽게 처리\n",
    "    padding = int(min(bbox['Width'], bbox['Height']) * width * 0.1)\n",
    "    draw.rectangle([\n",
    "        left - padding,\n",
    "        top - padding,\n",
    "        right + padding,\n",
    "        bottom + padding\n",
    "    ], fill=(0, 0, 0))\n",
    "\n",
    "# 전체 마스크에 블러 효과 적용\n",
    "mask = mask.filter(ImageFilter.GaussianBlur(radius=2))\n",
    "    \n",
    "mask.save(\"mask.png\", format=\"PNG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6e2bc0-9f9e-412d-8aaf-562b5a868935",
   "metadata": {},
   "source": [
    "# inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4bf710b-4a41-4d70-a731-56248b5ba620",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_predictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sagemaker-user/Self-Study-Generative-AI/05-Image-Inpainting/inpainting.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ngfbyyfevkn2rl2.studio.us-west-2.sagemaker.aws/home/sagemaker-user/Self-Study-Generative-AI/05-Image-Inpainting/inpainting.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m accept \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mapplication/json;jpeg\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ngfbyyfevkn2rl2.studio.us-west-2.sagemaker.aws/home/sagemaker-user/Self-Study-Generative-AI/05-Image-Inpainting/inpainting.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Note that sending or receiving payload with raw/rgb values may hit default limits for the input payload and the response size.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ngfbyyfevkn2rl2.studio.us-west-2.sagemaker.aws/home/sagemaker-user/Self-Study-Generative-AI/05-Image-Inpainting/inpainting.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m query_response \u001b[39m=\u001b[39m query(model_predictor, json\u001b[39m.\u001b[39mdumps(payload)\u001b[39m.\u001b[39mencode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m), content_type, accept)\n\u001b[1;32m     <a href='vscode-notebook-cell://ngfbyyfevkn2rl2.studio.us-west-2.sagemaker.aws/home/sagemaker-user/Self-Study-Generative-AI/05-Image-Inpainting/inpainting.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m generated_images \u001b[39m=\u001b[39m parse_response(query_response)\n\u001b[1;32m     <a href='vscode-notebook-cell://ngfbyyfevkn2rl2.studio.us-west-2.sagemaker.aws/home/sagemaker-user/Self-Study-Generative-AI/05-Image-Inpainting/inpainting.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# For accept = 'application/json;jpeg' mentioned above, returned image is a jpeg as bytes encoded with base64.b64 encoding.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ngfbyyfevkn2rl2.studio.us-west-2.sagemaker.aws/home/sagemaker-user/Self-Study-Generative-AI/05-Image-Inpainting/inpainting.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# Here, we decode the image and display the image.\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_predictor' is not defined"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import random\n",
    "\n",
    "\n",
    "# content_type = 'application/json;jpeg', endpoint expects payload to be a json with the original image and the mask image as bytes encoded with base64.b64 encoding.\n",
    "# To send raw image to the endpoint, you can set content_type = 'application/json' and encoded_image as np.array(PIL.Image.open(input_img_file_name.jpg)).tolist()\n",
    "content_type = \"application/json;jpeg\"\n",
    "\n",
    "\n",
    "with open(portrait_image, \"rb\") as f:\n",
    "    input_img_image_bytes = f.read()\n",
    "with open(\"./mask.png\", \"rb\") as f:\n",
    "    input_img_mask_image_bytes = f.read()\n",
    "\n",
    "encoded_input_image = base64.b64encode(bytearray(input_img_image_bytes)).decode()\n",
    "encoded_mask = base64.b64encode(bytearray(input_img_mask_image_bytes)).decode()\n",
    "\n",
    "prompt = \"Leonardo da Vinci painting style, In a Renaissance-era studio with an easel and oil painting tools, a man in his thirties gazes straight ahead. The painting style is that of Renaissance oil painting\"\n",
    "\n",
    "payload = {\n",
    "    \"prompt\": prompt,\n",
    "    \"image\": encoded_input_image,\n",
    "    \"mask_image\": encoded_mask,\n",
    "    \"num_inference_steps\": 50,\n",
    "    \"guidance_scale\": 12,\n",
    "    \"seed\": random.randint(0, 10000),\n",
    "    \"negative_prompt\": \"(deformed, distorted, disfigured), bad anatomy, bad proportions, extra limbs, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, mutated, blurry, ugly, oversaturated, grain, low-res, Deformed, bad anatomy, disfigured, poorly drawn face, mutated, extra limb, ugly, poorly drawn hands, missing limb, floating limbs, disconnected limbs, malformed hands, blur, out of focus, long neck, long body, plastic, 3d render, (((duplicate))), ((morbid)), ((mutilated)), extra fingers, mutated hands, ((poorly drawn hands)), ((poorly drawn face)), (((mutation))), (((deformed))), ((ugly)), blurry, ((bad anatomy)), (((bad proportions))), ((extra limbs)), cloned face, glitchy, out of frame, gross proportions, (malformed limbs), ((missing arms)), ((missing legs)), (((extra arms))), (((extra legs))), mutated hands, (fused fingers), (too many fingers), (((long neck))), low quality, low resolution, artifacts, watermark\",\n",
    "}\n",
    "\n",
    "\n",
    "# For accept = 'application/json;jpeg', endpoint returns the jpeg image as bytes encoded with base64.b64 encoding.\n",
    "# To receive raw image with rgb value set Accept = 'application/json'\n",
    "accept = \"application/json;jpeg\"\n",
    "\n",
    "# Note that sending or receiving payload with raw/rgb values may hit default limits for the input payload and the response size.\n",
    "\n",
    "query_response = query(model_predictor, json.dumps(payload).encode(\"utf-8\"), content_type, accept)\n",
    "generated_images = parse_response(query_response)\n",
    "\n",
    "\n",
    "# For accept = 'application/json;jpeg' mentioned above, returned image is a jpeg as bytes encoded with base64.b64 encoding.\n",
    "# Here, we decode the image and display the image.\n",
    "for generated_image in generated_images:\n",
    "    generated_image_decoded = BytesIO(base64.b64decode(generated_image.encode()))\n",
    "    generated_image_rgb = Image.open(generated_image_decoded).convert(\"RGB\")\n",
    "    # You can save the generated image by calling generated_image_rgb.save('inpainted_image.jpg')\n",
    "    display_img_and_prompt(generated_image_rgb, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cf6cc8-1071-4466-9671-f03177a4217d",
   "metadata": {},
   "source": [
    "# (test) bedrock - titan inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd09c0b-c3ed-4a25-b988-3e34f98fe3eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "import boto3\n",
    "from PIL import Image\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "class ImageError(Exception):\n",
    "    \"Custom exception for errors returned by Amazon Titan Image Generator G1\"\n",
    "\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "def generate_image(model_id, body):\n",
    "    \"\"\"\n",
    "    Generate an image using Amazon Titan Image Generator G1 model on demand.\n",
    "    Args:\n",
    "        model_id (str): The model ID to use.\n",
    "        body (str) : The request body to use.\n",
    "    Returns:\n",
    "        image_bytes (bytes): The image generated by the model.\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\n",
    "        \"Generating image with Amazon Titan Image Generator G1 model %s\", model_id)\n",
    "\n",
    "    bedrock = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "    accept = \"application/json\"\n",
    "    content_type = \"application/json\"\n",
    "\n",
    "    response = bedrock.invoke_model(\n",
    "        body=body, modelId=model_id, accept=accept, contentType=content_type\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "    base64_image = response_body.get(\"images\")[0]\n",
    "    base64_bytes = base64_image.encode('ascii')\n",
    "    image_bytes = base64.b64decode(base64_bytes)\n",
    "\n",
    "    finish_reason = response_body.get(\"error\")\n",
    "\n",
    "    if finish_reason is not None:\n",
    "        raise ImageError(f\"Image generation error. Error is {finish_reason}\")\n",
    "\n",
    "    logger.info(\n",
    "        \"Successfully generated image with Amazon Titan Image Generator G1 model %s\", model_id)\n",
    "\n",
    "    return image_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6e4838-5402-4519-9dc7-a865b97f8c7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"./source.png\", 'rb') as image_file:\n",
    "    image_bytes = image_file.read()\n",
    "    \n",
    "# 원본 이미지 열기\n",
    "image = Image.open(io.BytesIO(image_bytes))\n",
    "width, height = image.size\n",
    "\n",
    "mask = Image.new('L', (width, height), 255)\n",
    "draw = ImageDraw.Draw(mask)\n",
    "\n",
    "# Rekognition으로 얼굴 감지\n",
    "response = rekognition.detect_faces(\n",
    "    Image={'Bytes': image_bytes},\n",
    "    Attributes=['DEFAULT']\n",
    ")\n",
    "\n",
    "# 감지된 각 얼굴에 대해 마스크 생성\n",
    "for face_detail in response['FaceDetails']:\n",
    "    bbox = face_detail['BoundingBox']\n",
    "\n",
    "    # 바운딩 박스 좌표 계산\n",
    "    left = int(bbox['Left'] * width)\n",
    "    top = int(bbox['Top'] * height)\n",
    "    right = int((bbox['Left'] + bbox['Width']) * width)\n",
    "    bottom = int((bbox['Top'] + bbox['Height']) * height)\n",
    "\n",
    "    # 얼굴 영역을 검은색으로 채우기\n",
    "    draw.rectangle([left, top, right, bottom], fill=0)\n",
    "    \n",
    "mask.save(\"mask.png\", format=\"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c870ed7-a071-4e02-8624-5b6fef85195d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "    model_id = 'amazon.titan-image-generator-v2:0'\n",
    "\n",
    "    # Read image and mask image from file and encode as base64 strings.\n",
    "    with open(\"./source.png\", \"rb\") as image_file:\n",
    "        input_image = base64.b64encode(image_file.read()).decode('utf8')\n",
    "    with open(\"./mask.png\", \"rb\") as mask_image_file:\n",
    "        input_mask_image = base64.b64encode(\n",
    "            mask_image_file.read()).decode('utf8')\n",
    "\n",
    "    body = json.dumps({\n",
    "        \"taskType\": \"OUTPAINTING\",\n",
    "        \"outPaintingParams\": {\n",
    "            \"text\": \"In front of the Eiffel Tower in Paris, wearing a beret and holding a baguette, warm sunset, romantic atmosphere, 8k quality\",\n",
    "            \"negativeText\": \"bad quality, low res\",\n",
    "            \"image\": input_image,\n",
    "            \"maskImage\": input_mask_image,\n",
    "            \"outPaintingMode\": \"DEFAULT\"\n",
    "        },\n",
    "        \"imageGenerationConfig\": {\n",
    "            \"numberOfImages\": 1,\n",
    "            \"height\": 1408,\n",
    "            \"width\": 640,\n",
    "            \"cfgScale\": 7.5\n",
    "        }\n",
    "    }\n",
    "    )\n",
    "\n",
    "    image_bytes = generate_image(model_id=model_id,\n",
    "                                 body=body)\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "    image.show()\n",
    "\n",
    "except ClientError as err:\n",
    "    message = err.response[\"Error\"][\"Message\"]\n",
    "    logger.error(\"A client error occurred: %s\", message)\n",
    "    print(\"A client error occured: \" +\n",
    "          format(message))\n",
    "except ImageError as err:\n",
    "    logger.error(err.message)\n",
    "    print(err.message)\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        f\"Finished generating image with Amazon Titan Image Generator G1 model {model_id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695504ac-5107-437d-af3a-53493ed562ba",
   "metadata": {},
   "source": [
    "# (Test2) rekognition -> semantic segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac643e89-3c79-4e3d-a693-384926ef8b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "s3_bucket = f\"jumpstart-cache-prod-{region}\"\n",
    "key_prefix = \"inference-notebook-assets\"\n",
    "s3 = boto3.client(\"s3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce0347e-7a5e-451d-8ccd-5abdf2620168",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "def query_endpoint(input_img):\n",
    "    endpoint_name = 'jumpstart-dft-mx-semseg-fcn-resnet1-20241025-011704'\n",
    "    client = boto3.client('runtime.sagemaker')\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name, ContentType='application/x-image', Body=input_img, Accept='application/json;verbose')\n",
    "    return response\n",
    "\n",
    "def parse_seg_response(query_response):\n",
    "    response_dict = json.loads(query_response['Body'].read())\n",
    "    return response_dict['predictions'],response_dict['labels'], response_dict['image_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0325bc4b-9fb4-4250-9292-be0eb1f58a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_jpg = \"source5.png\"\n",
    "\n",
    "with open(img_jpg, 'rb') as file: input_img = file.read()\n",
    "\n",
    "try:\n",
    "    query_response = query_endpoint(input_img)\n",
    "except Exception as e:\n",
    "    if e.response['Error']['Code'] == 'ModelError':\n",
    "        raise Exception(\n",
    "             \"Backend scripts have been updated in Feb '22 to standardize response \"\n",
    "             \"format of endpoint response.\"\n",
    "             \"Previous endpoints may not support verbose response type used in this notebook.\"\n",
    "             f\"To use this notebook, please launch the endpoint again. Error: {e}.\"\n",
    "        )\n",
    "    else:\n",
    "        raise\n",
    "try:\n",
    "    predictions, labels, image_labels =  parse_seg_response(query_response)\n",
    "except (TypeError, KeyError) as e:\n",
    "    raise Exception(\n",
    "          \"Backend scripts have been updated in Feb '22 to standardize response \"\n",
    "          \"format of endpoint response.\"\n",
    "           \"Response from previous endpoints not consistent with this notebook.\"\n",
    "           f\"To use this notebook, please launch the endpoint again. Error: {e}.\"\n",
    "   )\n",
    "print('Objects present in the picture:',image_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4fd0d8-b719-42d8-af3c-d2bb850681c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getvocpallete(num_cls):\n",
    "    # 이진 팔레트 생성 (검은색과 흰색만 사용)\n",
    "    pallete = [0] * (num_cls * 3)\n",
    "    \n",
    "    # 배경(255)은 흰색으로 설정\n",
    "    pallete[255*3:255*3+3] = [255, 255, 255]\n",
    "    \n",
    "    # person 클래스(15)는 검은색으로 설정 - 이미 0으로 초기화되어 있음\n",
    "    # pallete[15*3:15*3+3] = [0, 0, 0]\n",
    "    \n",
    "    return pallete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a1a248-d70e-4838-bddf-4f320f95adcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pallete = getvocpallete(256)\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "npimg = np.array(predictions)\n",
    "npimg[npimg != 15] = 255\n",
    "npimg[npimg == -1] = 255\n",
    "mask = Image.fromarray(npimg.astype('uint8'))\n",
    "mask.putpalette(pallete)\n",
    "print(mask.size)\n",
    "plt.imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1039e4ab-df27-4bf2-ba47-1893eb305bc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cropped = mask.crop((0, 0, 512, 512))\n",
    "cropped.save(\"mask.png\")\n",
    "cropped.show()\n",
    "\n",
    "# cropped + mask된 이미지를 다시 stable diffusion 또는 bedrock 으로.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
