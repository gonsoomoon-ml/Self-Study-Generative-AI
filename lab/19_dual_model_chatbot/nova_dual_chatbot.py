#!/usr/bin/env python3
"""
Amazon Bedrock Nova ë“€ì–¼ ëª¨ë¸ ì±—ë´‡
Nova Micro: ì¦‰ê°ì ì¸ ì´ˆê¸° ì‘ë‹µ
Nova Pro: ìƒì„¸í•œ ìµœì¢… ë‹µë³€

GitHub: https://github.com/your-username/amazon-nova-dual-chatbot
"""

import boto3
import json
import sys
import time
from concurrent.futures import ThreadPoolExecutor
from botocore.config import Config

class NovaDualChatbot:
    def __init__(self, region='us-east-1'):
        """
        Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        
        Args:
            region (str): AWS ë¦¬ì „ (ê¸°ë³¸ê°’: us-east-1)
        """
        # íƒ€ì„ì•„ì›ƒ ì„¤ì •ìœ¼ë¡œ ì•ˆì •ì„± í–¥ìƒ
        config = Config(
            read_timeout=60,
            connect_timeout=10,
            retries={'max_attempts': 3}
        )
        
        try:
            self.bedrock_runtime = boto3.client(
                service_name='bedrock-runtime',
                region_name=region,
                config=config
            )
            print(f"âœ… Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (ë¦¬ì „: {region})")
        except Exception as e:
            print(f"âŒ Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ AWS ìê²© ì¦ëª…ê³¼ ë¦¬ì „ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            sys.exit(1)
    
    def stream_nova_micro(self, prompt: str):
        """
        Nova Micro ëª¨ë¸ ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ - ì¦‰ê°ì ì¸ ì´ˆê¸° ì‘ë‹µ
        
        Args:
            prompt (str): ì…ë ¥ í”„ë¡¬í”„íŠ¸
            
        Yields:
            str: Nova Microì˜ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²­í¬
        """
        model_id = 'amazon.nova-micro-v1:0'
        
        # Nova Microìš© ìš”ì²­ ë³¸ë¬¸
        body = {
            "messages": [
                {
                    "role": "user",
                    "content": [{"text": prompt}]
                }
            ],
            "inferenceConfig": {
                "max_new_tokens": 500,  # 400ì—ì„œ 500ìœ¼ë¡œ ì¦ê°€
                "temperature": 0.3
            }
        }

        try:
            response = self.bedrock_runtime.invoke_model_with_response_stream(
                body=json.dumps(body),
                modelId=model_id,
                accept='application/json',
                contentType='application/json'
            )
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
            stream = response.get('body')
            if stream:
                for event in stream:
                    chunk = event.get('chunk')
                    if chunk:
                        chunk_obj = json.loads(chunk.get('bytes').decode())
                        
                        # contentBlockDeltaì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        if 'contentBlockDelta' in chunk_obj:
                            delta = chunk_obj['contentBlockDelta'].get('delta', {})
                            text_content = delta.get('text', '')
                            if text_content:
                                yield text_content
                                
        except Exception as e:
            yield f"âŒ Nova Micro ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}"

    def stream_nova_pro(self, prompt: str):
        """
        Nova Pro ëª¨ë¸ ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ - ìƒì„¸í•œ ìµœì¢… ë‹µë³€
        
        Args:
            prompt (str): ì…ë ¥ í”„ë¡¬í”„íŠ¸
            
        Yields:
            str: Nova Proì˜ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²­í¬
        """
        model_id = 'amazon.nova-pro-v1:0'
        
        # Nova Proìš© ìš”ì²­ ë³¸ë¬¸
        body = {
            "messages": [
                {
                    "role": "user", 
                    "content": [{"text": prompt}]
                }
            ],
            "inferenceConfig": {
                "max_new_tokens": 2048,
                "temperature": 0.5
            }
        }

        try:
            print("ğŸƒâ€â™€ï¸ Nova Pro ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘...")
            
            response = self.bedrock_runtime.invoke_model_with_response_stream(
                body=json.dumps(body),
                modelId=model_id,
                accept='application/json',
                contentType='application/json'
            )
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
            stream = response.get('body')
            if stream:
                for event in stream:
                    chunk = event.get('chunk')
                    if chunk:
                        chunk_obj = json.loads(chunk.get('bytes').decode())
                        
                        # contentBlockDeltaì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        if 'contentBlockDelta' in chunk_obj:
                            delta = chunk_obj['contentBlockDelta'].get('delta', {})
                            text_content = delta.get('text', '')
                            if text_content:
                                yield text_content

        except Exception as e:
            yield f"âŒ Nova Pro ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}"

    def create_prompts(self, user_query: str):
        """
        ê° ëª¨ë¸ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„±
        
        Args:
            user_query (str): ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            tuple: (micro_prompt, pro_prompt)
        """
        micro_prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ê°„ê²°í•˜ê³  ì™„ê²°ëœ ê°œìš”ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”: "{user_query}"

ìš”êµ¬ì‚¬í•­:
- í•µì‹¬ ë‚´ìš©ì„ 1-2ê°œ ë¬¸ë‹¨ìœ¼ë¡œ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ì„¤ëª… (ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ)
- ë°˜ë“œì‹œ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ë§ˆë¬´ë¦¬í•  ê²ƒ
- ë§ˆì§€ë§‰ì— "ë” êµ¬ì²´ì ìœ¼ë¡œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤." ê°™ì€ ê°„ë‹¨í•œ ì—°ê²° ë¬¸êµ¬ë¡œ ë§ˆë¬´ë¦¬
- í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”

ì¤‘ìš”: ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì—¬ ë‹µë³€ì´ ì¤‘ê°„ì— ëŠì–´ì§€ì§€ ì•Šë„ë¡ í•´ì£¼ì„¸ìš”."""

        pro_prompt = f"""ì•ì„œ "{user_query}"ì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ê°œìš”ë¥¼ ì œê³µí–ˆìŠµë‹ˆë‹¤. 
ì´ì œ ê·¸ ë‚´ìš©ì— ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì„œ ë” ìƒì„¸í•˜ê³  ì „ë¬¸ì ì¸ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
- ì•ì„  ë‹µë³€ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ì—°ì¥ì„ ìƒì—ì„œ ì‹œì‘ (ì˜ˆ: "ë” êµ¬ì²´ì ìœ¼ë¡œ ì‚´í´ë³´ë©´...", "ì´ë¥¼ ë” ìì„¸íˆ ë¶„ì„í•˜ë©´..." ë“±)
- êµ¬ì²´ì ì¸ ì˜ˆì‹œ, ì¥ë‹¨ì , ì‚¬ìš© ì‚¬ë¡€, ë¹„êµ ë¶„ì„ì„ í¬í•¨í•œ ì‹¬í™” ë‚´ìš©
- ë§ˆí¬ë‹¤ìš´ì„ í™œìš©í•œ ì²´ê³„ì ì´ê³  êµ¬ì¡°í™”ëœ ë‹µë³€
- ì‹¤ë¬´ì ì´ê³  ì „ë¬¸ì ì¸ ê´€ì ì—ì„œì˜ ê¹Šì´ ìˆëŠ” ì„¤ëª…
- í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”

ë‹µë³€ ìŠ¤íƒ€ì¼: ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰½ê²Œ, ë…¼ë¦¬ì ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ì™„ì „í•œ ê°€ì´ë“œê°€ ë˜ë„ë¡"""

        return micro_prompt, pro_prompt

    def chat(self, user_query: str):
        """
        ë“€ì–¼ ëª¨ë¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë©”ì¸ í•¨ìˆ˜
        
        Args:
            user_query (str): ì‚¬ìš©ì ì§ˆë¬¸
        """
        print(f"\nğŸ’¬ ì§ˆë¬¸: {user_query}")
        print("=" * 60)
        
        # ê° ëª¨ë¸ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±
        micro_prompt, pro_prompt = self.create_prompts(user_query)
        
        print("ğŸ¤– ë‹µë³€:")
        
        # Pro ëª¨ë¸ì˜ ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ë¥¼ ì €ì¥í•  ë²„í¼
        pro_buffer = []
        pro_complete = False
        
        def collect_pro_stream():
            """Pro ëª¨ë¸ì˜ ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ë¥¼ ë²„í¼ì— ìˆ˜ì§‘"""
            nonlocal pro_buffer, pro_complete
            try:
                for chunk in self.stream_nova_pro(pro_prompt):
                    pro_buffer.append(chunk)
                pro_complete = True
            except Exception as e:
                pro_buffer.append(f"\nâŒ Nova Pro ì˜¤ë¥˜: {e}")
                pro_complete = True
        
        # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=2) as executor:
            # Pro ëª¨ë¸ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘
            future_pro = executor.submit(collect_pro_stream)
            
            # 1ë‹¨ê³„: Nova Micro ìŠ¤íŠ¸ë¦¬ë° (ë¹ ë¥¸ ì´ˆê¸° ì‘ë‹µ)
            print("ğŸƒâ€â™‚ï¸ Nova Micro ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘...")
            try:
                for chunk in self.stream_nova_micro(micro_prompt):
                    sys.stdout.write(chunk)
                    sys.stdout.flush()
                    time.sleep(0.02)  # ìì—°ìŠ¤ëŸ¬ìš´ íƒ€ì´í•‘ íš¨ê³¼
                
                # Micro ì™„ë£Œ í›„ ì¤„ë°”ê¿ˆ
                print("\n")
                
            except Exception as e:
                print(f"âŒ Nova Micro ì˜¤ë¥˜: {e}")
                print("\n")
            
            # 2ë‹¨ê³„: Pro ëª¨ë¸ ê²°ê³¼ ì¶œë ¥ (ë²„í¼ì—ì„œ ê°€ì ¸ì™€ì„œ ìŠ¤íŠ¸ë¦¬ë°ì²˜ëŸ¼ ì¶œë ¥)
            buffer_index = 0
            
            while not pro_complete or buffer_index < len(pro_buffer):
                # ë²„í¼ì— ìƒˆë¡œìš´ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì¶œë ¥
                while buffer_index < len(pro_buffer):
                    chunk = pro_buffer[buffer_index]
                    sys.stdout.write(chunk)
                    sys.stdout.flush()
                    time.sleep(0.01)  # ì•½ê°„ ë” ë¹ ë¥¸ íƒ€ì´í•‘ íš¨ê³¼
                    buffer_index += 1
                
                # Pro ëª¨ë¸ì´ ì•„ì§ ì™„ë£Œë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì ì‹œ ëŒ€ê¸°
                if not pro_complete:
                    time.sleep(0.05)
            
            # Pro ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
            future_pro.result()
        
        print(f"\n\nâœ… ì‘ë‹µ ì™„ë£Œ!")
        print("=" * 60)

def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("ğŸ¤– Amazon Nova ë“€ì–¼ ëª¨ë¸ ì±—ë´‡")
    print("Nova Micro + Nova Pro ì¡°í•©ìœ¼ë¡œ ë¹ ë¥´ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.")
    print("=" * 60)
    
    # ì±—ë´‡ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    try:
        chatbot = NovaDualChatbot()
    except Exception as e:
        print(f"âŒ ì±—ë´‡ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_queries = [
        "AWS Lambdaì™€ EC2ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ì˜¤ë²„í”¼íŒ…ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
        "Pythonì˜ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì€?",
        "í´ë¼ìš°ë“œ ì»´í“¨íŒ…ì˜ ì¥ì ì„ ì•Œë ¤ì£¼ì„¸ìš”."
    ]
    
    # ëŒ€í™”í˜• ëª¨ë“œ ë˜ëŠ” í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì„ íƒ
    print("ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    print("1: ëŒ€í™”í˜• ëª¨ë“œ (ì§ì ‘ ì§ˆë¬¸ ì…ë ¥)")
    print("2: í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ë¯¸ë¦¬ ì¤€ë¹„ëœ ì§ˆë¬¸)")
    
    mode = input("ì„ íƒ (1 ë˜ëŠ” 2): ").strip()
    
    if mode == "1":
        # ëŒ€í™”í˜• ëª¨ë“œ
        print("\nğŸ’¡ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥):")
        while True:
            user_input = input("\nğŸ™‹â€â™‚ï¸ ì§ˆë¬¸: ").strip()
            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                print("ğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            if user_input:
                chatbot.chat(user_input)
    
    elif mode == "2":
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
        print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {len(test_queries)}ê°œ ì§ˆë¬¸ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
        for i, query in enumerate(test_queries, 1):
            print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ {i}/{len(test_queries)}")
            chatbot.chat(query)
            
            if i < len(test_queries):
                input("\nâ¸ï¸  ë‹¤ìŒ í…ŒìŠ¤íŠ¸ë¡œ ì§„í–‰í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
    
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 1 ë˜ëŠ” 2ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()
