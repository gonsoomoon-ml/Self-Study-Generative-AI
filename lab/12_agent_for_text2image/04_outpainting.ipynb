{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Agentic system for image generation - in/out painting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Setting\n",
    " - Auto Reload\n",
    " - path for utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "module_path = \"../..\"\n",
    "sys.path.append(os.path.abspath(module_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 1. Create Bedrock client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from termcolor import colored\n",
    "from utils import bedrock\n",
    "from utils.bedrock import bedrock_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "- os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "- os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "- os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "- os.environ[\"BEDROCK_ENDPOINT_URL\"] = \"<YOUR_ENDPOINT_URL>\"  # E.g. \"https://...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    endpoint_url=os.environ.get(\"BEDROCK_ENDPOINT_URL\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    ")\n",
    "\n",
    "print (colored(\"\\n== FM lists ==\", \"green\"))\n",
    "pprint (bedrock_info.get_list_fm_models(verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 2. LLM 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.bedrock import bedrock_model\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = bedrock_model(\n",
    "    model_id=bedrock_info.get_model_id(model_name=\"Claude-V3-7-Sonnet-CRI\"),\n",
    "    bedrock_client=boto3_bedrock,\n",
    "    stream=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    inference_config={\n",
    "        'maxTokens': 1024,\n",
    "        'stopSequences': [\"\\n\\nHuman\"],\n",
    "        'temperature': 0.01,\n",
    "        #'topP': ...,\n",
    "    }\n",
    "    #additional_model_request_fields={\"top_k\": 200}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_generation_model = bedrock_model(\n",
    "    model_id=bedrock_info.get_model_id(model_name=\"Nova-Canvas\"),\n",
    "    bedrock_client=boto3_bedrock\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 3. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import pprint\n",
    "import base64\n",
    "import traceback\n",
    "from PIL import Image\n",
    "from termcolor import colored\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from textwrap import dedent\n",
    "from utils.bedrock import bedrock_utils\n",
    "from typing import TypedDict\n",
    "from src.genai_anaysis import llm_call\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.runnables import RunnableConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TimeMeasurement:\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.measurements = {}\n",
    "\n",
    "    def start(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def measure(self, section_name):\n",
    "        if self.start_time is None:\n",
    "            raise ValueError(\"start() 메서드를 먼저 호출해야 합니다.\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - self.start_time\n",
    "        self.measurements[section_name] = elapsed_time\n",
    "        self.start_time = end_time  # 다음 구간 측정을 위해 시작 시간 재설정\n",
    "\n",
    "    def reset(self, ):\n",
    "        self.measurements = {}\n",
    "\n",
    "    def print_measurements(self):\n",
    "        for section, elapsed_time in self.measurements.items():\n",
    "            #print(f\"{section}: {elapsed_time:.5f} 초\")\n",
    "            print(colored (f\"\\nelapsed time: {section}: {elapsed_time:.5f} 초\", \"red\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### 3.1 Agent state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    ask: str\n",
    "    task_type: str\n",
    "    ask_repo: str\n",
    "    origin_ask_repo: str\n",
    "    prompt_components: dict\n",
    "    image_prompt: dict\n",
    "    image_model: str\n",
    "    generated_img_path: str\n",
    "    suggestions: str\n",
    "    retouch: str\n",
    "    retry_count: int\n",
    "    control_image_needed: str\n",
    "    control_mode: str\n",
    "    mask_image: str\n",
    "    prev_node: str\n",
    "    original_image: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GenAIOutPainting():\n",
    "    \"\"\"A class for editing images.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"Initialize the GenAIOutPainting with required models and components.\n",
    "        \n",
    "        Args:\n",
    "            **kwargs: Keyword arguments including llm and image_generation_model.\n",
    "        \"\"\"\n",
    "        self.llm=kwargs[\"llm\"]\n",
    "        self.image_generation_model = kwargs[\"image_generation_model\"]\n",
    "        self.state = GraphState\n",
    "\n",
    "        self.llm_caller = llm_call(\n",
    "            llm=self.llm,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        self._graph_definition()\n",
    "        self.messages = []\n",
    "\n",
    "        self.timer = TimeMeasurement()\n",
    "\n",
    "    def _get_string_from_message(self, message):\n",
    "        \"\"\"Extract text content from a message.\n",
    "        \n",
    "        Args:\n",
    "            message: A message dictionary containing content.\n",
    "            \n",
    "        Returns:\n",
    "            str: The text content from the message.\n",
    "        \"\"\"\n",
    "        return message[\"content\"][0][\"text\"]\n",
    "\n",
    "    def _get_message_from_string(self, role, string, imgs=None):\n",
    "        \"\"\"Create a message dictionary from text and optional images.\n",
    "        \n",
    "        Args:\n",
    "            role: The role of the message sender.\n",
    "            string: The text content.\n",
    "            imgs: Optional list of images to include.\n",
    "            \n",
    "        Returns:\n",
    "            dict: A formatted message dictionary.\n",
    "        \"\"\"\n",
    "        message = {\n",
    "            \"role\": role,\n",
    "            \"content\": []\n",
    "        }\n",
    "        \n",
    "        if imgs is not None:\n",
    "            for img in imgs:\n",
    "                img_message = {\n",
    "                    \"image\": {\n",
    "                        \"format\": 'png',\n",
    "                        \"source\": {\"bytes\": img}\n",
    "                    }\n",
    "                }\n",
    "                message[\"content\"].append(img_message)\n",
    "        \n",
    "        message[\"content\"].append({\"text\": dedent(string)})\n",
    "\n",
    "        return message\n",
    "    \n",
    "    def _png_to_bytes(self, file_path):\n",
    "        \"\"\"Convert a PNG file to binary data and base64 string.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the PNG file.\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (binary_data, base64_string) or error message.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, \"rb\") as image_file:\n",
    "                # Read file in binary mode\n",
    "                binary_data = image_file.read()\n",
    "                \n",
    "                # Encode binary data to base64\n",
    "                base64_encoded = base64.b64encode(binary_data)\n",
    "                \n",
    "                # Decode bytes to string\n",
    "                base64_string = base64_encoded.decode('utf-8')\n",
    "                \n",
    "                return binary_data, base64_string\n",
    "                \n",
    "        except FileNotFoundError:\n",
    "            return \"Error: 파일을 찾을 수 없습니다.\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def show_save_image(self, base64_string):\n",
    "        \"\"\"Display and save an image from base64 string.\n",
    "        \n",
    "        Args:\n",
    "            base64_string: Base64 encoded image data.\n",
    "            \n",
    "        Returns:\n",
    "            str: Path to the saved image file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            \n",
    "            # Decode base64 string to binary data\n",
    "            image_data = base64.b64decode(base64_string)\n",
    "            \n",
    "            # Convert binary data to image\n",
    "            image = Image.open(io.BytesIO(image_data))\n",
    "\n",
    "            fig, (ax1) = plt.subplots(1, 1, figsize=(15, 7))\n",
    "            ax1.imshow(image)\n",
    "            ax1.axis('off')\n",
    "            ax1.set_title('Generated Image')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # save images\n",
    "            img_path = self.file_name\n",
    "            image.save(img_path, \"PNG\")\n",
    "            \n",
    "            return img_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: 이미지를 표시하는 데 실패했습니다. {str(e)}\")\n",
    "            \n",
    "    def _body_generator(\n",
    "            self, image_prompt, taskType=\"TEXT_IMAGE\", maskImage=None,\n",
    "            original_image=None,\n",
    "    ):    \n",
    "        \"\"\"Generate request body for image generation API.\n",
    "        \n",
    "        Args:\n",
    "            image_prompt: Dictionary containing main_prompt and negative_prompt.\n",
    "            taskType: Type of image generation task.\n",
    "            maskImage: Optional mask image for outpainting.\n",
    "            original_image: Optional original image for outpainting.\n",
    "            \n",
    "        Returns:\n",
    "            str: JSON string for the request body.\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If required parameters are missing for specific task types.\n",
    "        \"\"\"\n",
    "        if taskType == \"OUTPAINTING\":\n",
    "            # Create request body for outpainting\n",
    "            if maskImage is not None:\n",
    "                # Read mask image file\n",
    "                _, mask_base64 = self._png_to_bytes(maskImage)\n",
    "                _, img_base64 = self._png_to_bytes(original_image)\n",
    "                \n",
    "                body_dict = {\n",
    "                    \"taskType\": \"OUTPAINTING\",\n",
    "                    \"outPaintingParams\": {\n",
    "                        \"image\": img_base64,  # Original image\n",
    "                        \"maskImage\": mask_base64,  # Mask image\n",
    "                        \"text\": image_prompt[\"main_prompt\"],\n",
    "                        \"outPaintingMode\": \"PRECISE\",\n",
    "                        \"negativeText\": image_prompt[\"negative_prompt\"]\n",
    "                    },\n",
    "                    \"imageGenerationConfig\": {\n",
    "                        \"numberOfImages\": 1,\n",
    "                        \"quality\": \"premium\",\n",
    "                        \"cfgScale\": 10,\n",
    "                        \"seed\": 12,\n",
    "                    }\n",
    "                }\n",
    "            else:\n",
    "                raise ValueError(\"maskImage는 OUTPAINTING 작업 유형에 필수입니다.\")\n",
    "        else:\n",
    "            raise ValueError(\"유효하지 않은 taskType입니다. 'OUTPAINTING' 이어야 합니다.\")\n",
    "    \n",
    "        return json.dumps(body_dict)\n",
    "\n",
    "    def get_messages(self, ):\n",
    "        return self.messages\n",
    "        \n",
    "    def _graph_definition(self, **kwargs):\n",
    "        \"\"\"Define the workflow graph for image generation.\n",
    "        \n",
    "        Args:\n",
    "            **kwargs: Additional keyword arguments.\n",
    "        \"\"\"\n",
    "\n",
    "        def ask_reformulation(state):\n",
    "            \"\"\"Reformulate user's ask into an optimized request.\n",
    "\n",
    "            Args:\n",
    "                state: Current state dictionary.\n",
    "                \n",
    "            Returns:\n",
    "                Updated state with reformulated ask.\n",
    "            \"\"\"\n",
    "\n",
    "            self.timer.start()\n",
    "            self.timer.reset()\n",
    "            \n",
    "            print(\"---ASK REFORMULATION---\")\n",
    "            ask = state[\"ask\"]\n",
    "            image_prompt = state.get(\"image_prompt\", \"None\")\n",
    "            origin_ask_repo = state.get(\"origin_ask_repo\", \"None\")\n",
    "            messages = []\n",
    "            \n",
    "            print (\"image_prompt\", image_prompt)\n",
    "            \n",
    "            system_prompts = dedent(\n",
    "                '''\n",
    "                <task>\n",
    "                당신은 GenAI 활용 Outpainting 전문가입니다. 사용자의 원본 요청(ask), 기존 프롬프트(main_prompt/negative_prompt)을 통합하여 최적화된 단일 요청으로 재구성하세요.\n",
    "                main_prompt: \"A text prompt that describes what to generate within the masked region. If you omit this field, the model will remove elements inside the masked area. They will be replaced with a seamless extension of the image background\"\n",
    "                negative_prompt: \"A text prompt to define what not to include in the image.\"\n",
    "                </task>\n",
    "            \n",
    "                <instruction>\n",
    "                우선순위에 따라 요청을 체계적으로 재구성하세요:\n",
    "                1. 사용자의 원본 요청(ask)이 최우선 고려사항입니다.\n",
    "                2. 모든 요소들이 자연스럽게 통합되도록 조정하세요.\n",
    "            \n",
    "                </instruction>\n",
    "            \n",
    "                <output_format>\n",
    "                반드시 다음 형식의 JSON만 반환하고 설명이나 추가 텍스트를 포함하지 마세요:\n",
    "                \n",
    "                {\n",
    "                    \"ask_repo\": \"재구성된 요청\",\n",
    "                }\n",
    "                </output_format>\n",
    "                '''\n",
    "            )\n",
    "            \n",
    "            system_prompts = bedrock_utils.get_system_prompt(system_prompts=system_prompts)\n",
    "            user_prompts = dedent(\n",
    "                '''\n",
    "                Here is user's ask: <ask>{ask}</ask>\n",
    "                Here is main_prompt: <main_prompt>{main_prompt}</main_prompt>\n",
    "                Here is negative_prompt: <negative_prompt>{negative_prompt}</negative_prompt>\n",
    "                '''\n",
    "            )\n",
    "            context = {\n",
    "                \"ask\": ask,\n",
    "                \"main_prompt\": image_prompt[\"main_prompt\"] if image_prompt != \"None\" else \"None\",\n",
    "                \"negative_prompt\": image_prompt[\"negative_prompt\"] if image_prompt != \"None\" else \"None\",\n",
    "            }\n",
    "            user_prompts = user_prompts.format(**context)\n",
    "                       \n",
    "            message = self._get_message_from_string(role=\"user\", string=user_prompts)\n",
    "            self.messages.append(message)\n",
    "            messages.append(message)\n",
    "            \n",
    "            resp, ai_message = self.llm_caller.invoke(messages=messages, system_prompts=system_prompts)\n",
    "            self.messages.append(ai_message)\n",
    "            results = eval(resp['text'])\n",
    "            ask_repo = results[\"ask_repo\"]\n",
    "            \n",
    "            if origin_ask_repo == \"None\": origin_ask_repo = ask_repo\n",
    "            \n",
    "            return self.state(\n",
    "                ask_repo=ask_repo,\n",
    "                origin_ask_repo=origin_ask_repo,\n",
    "                prev_node=\"ASK_REFORMULATION\")   \n",
    "            \n",
    "        def check_readness_prompt_generation(state):\n",
    "            \"\"\"Check readiness for prompt generation by extracting visual components.\n",
    "            \n",
    "            Args:\n",
    "                state: Current state dictionary.\n",
    "                \n",
    "            Returns:\n",
    "                Updated state with extracted prompt components.\n",
    "            \"\"\"\n",
    "            print(\"---CHECK READNESS FOR PROMPT GENERATION---\")\n",
    "            ask_repo = state[\"ask_repo\"]\n",
    "            image_prompt = state.get(\"image_prompt\", \"None\")\n",
    "            messages = []\n",
    "            print (\"image_prompt\", image_prompt)\n",
    "            \n",
    "            system_prompts = dedent(\n",
    "                '''\n",
    "                <task>\n",
    "                사용자의 이미지 생성 요청을 분석하여 6가지 핵심 시각적 요소를 정확히 추출하세요.\n",
    "                </task>\n",
    "            \n",
    "                <instruction>\n",
    "                다음 핵심 시각 요소들을 사용자 요청에서 식별하고 정확히 추출하세요:\n",
    "            \n",
    "                1. subject (주체): 이미지의 중심이 되는 인물, 물체 또는 개체\n",
    "                   - 예: \"여자\", \"고양이\", \"산\", \"도시 풍경\"\n",
    "            \n",
    "                2. action (행동): 주체가 취하는 동작이나 상태\n",
    "                   - 예: \"달리는\", \"웃고 있는\", \"떠오르는\", \"휴식 중인\"\n",
    "            \n",
    "                3. environment (환경): 배경 장소나 주변 환경\n",
    "                   - 예: \"해변\", \"도시\", \"우주\", \"숲속\"\n",
    "            \n",
    "                4. lighting (조명): 빛의 상태나 분위기\n",
    "                   - 예: \"일몰\", \"푸른 빛\", \"어두운\", \"밝고 화창한\"\n",
    "            \n",
    "                5. style (스타일): 예술적 표현 방식이나 참조\n",
    "                   - 예: \"수채화\", \"사실적\", \"애니메이션\", \"미니멀리즘\"\n",
    "            \n",
    "                6. camera_position (카메라 위치): 시점이나 프레이밍\n",
    "                   - 예: \"클로즈업\", \"조감도\", \"측면 각도\", \"넓은 샷\"\n",
    "            \n",
    "                각 요소가 요청에 명시적으로 언급되지 않은 경우 해당 필드는 None로 설정하세요.\n",
    "                관련 내용이 있다면 사용자의 원문을 최대한 그대로 추출하세요.\n",
    "                암시적으로 언급된 요소도 파악하여 추출하세요.\n",
    "                </instruction>\n",
    "            \n",
    "                <output_format>\n",
    "                반드시 다음 형식의 JSON만 반환하고 설명이나 추가 텍스트를 포함하지 마세요:\n",
    "                \n",
    "                {\n",
    "                    \"components\": {\n",
    "                        \"subject\": {\n",
    "                            \"content\": \"발견된 텍스트 또는 None\"\n",
    "                        },\n",
    "                        \"action\": {\n",
    "                            \"content\": \"발견된 텍스트 또는 None\"\n",
    "                        },\n",
    "                        \"environment\": {\n",
    "                            \"content\": \"발견된 텍스트 또는 None\"\n",
    "                        },\n",
    "                        \"lighting\": {\n",
    "                            \"content\": \"발견된 텍스트 또는 None\"\n",
    "                        },\n",
    "                        \"style\": {\n",
    "                            \"content\": \"발견된 텍스트 또는 None\"\n",
    "                        },\n",
    "                        \"camera_position\": {\n",
    "                            \"content\": \"발견된 텍스트 또는 None\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "                </output_format>\n",
    "                '''\n",
    "            )\n",
    "                \n",
    "            system_prompts = bedrock_utils.get_system_prompt(system_prompts=system_prompts)\n",
    "            user_prompts = dedent(\n",
    "                '''\n",
    "                Here is user's ask: <ask>{ask}</ask>\n",
    "                Here is main_prompt: <main_prompt>{main_prompt}</main_prompt>\n",
    "                Here is negative_prompt: <negative_prompt>{negative_prompt}</negative_prompt>\n",
    "                '''\n",
    "            )\n",
    "            context = {\n",
    "                \"ask\": ask_repo,\n",
    "                \"main_prompt\": image_prompt[\"main_prompt\"] if image_prompt != \"None\" else \"None\",\n",
    "                \"negative_prompt\": image_prompt[\"negative_prompt\"] if image_prompt != \"None\" else \"None\"                \n",
    "            }\n",
    "            user_prompts = user_prompts.format(**context)\n",
    "                       \n",
    "            message = self._get_message_from_string(role=\"user\", string=user_prompts)\n",
    "            self.messages.append(message)\n",
    "            messages.append(message)\n",
    "            \n",
    "            resp, ai_message = self.llm_caller.invoke(messages=messages, system_prompts=system_prompts)\n",
    "            self.messages.append(ai_message)\n",
    "            results = eval(resp['text'])\n",
    "            prompt_components = results[\"components\"]\n",
    "            \n",
    "            return self.state(prompt_components=prompt_components, prev_node=\"CHECK_READNESS_PROMPT_GENERATION\")\n",
    "                \n",
    "        def prompt_generation_for_image(state):\n",
    "            \"\"\"Generate optimized image prompts based on extracted components.\n",
    "            \n",
    "            Args:\n",
    "                state: Current state dictionary.\n",
    "                \n",
    "            Returns:\n",
    "                Updated state with generated image prompt.\n",
    "            \"\"\"\n",
    "            print(\"---PROMPT GENERATION FOR IMAGE---\")\n",
    "            ask_repo, prompt_components, image_model,  = state[\"ask_repo\"], state[\"prompt_components\"], state[\"image_model\"]\n",
    "            image_prompt = state.get(\"image_prompt\", \"None\")\n",
    "            messages = []\n",
    "            \n",
    "            system_prompts = dedent(\n",
    "                '''\n",
    "                <task>\n",
    "                추출된 시각 요소들을 활용하여 이미지 outpainting 모델 {image_model}에 최적화된 고품질 프롬프트를 생성하세요.\n",
    "                </task>\n",
    "            \n",
    "                <instruction>\n",
    "                이미지 outpainting 프롬프트 전문가로서, 다음 원칙에 따라 최적의 프롬프트를 구성하세요:\n",
    "            \n",
    "                1. 이미지 캡션 형태로 작성\n",
    "                   - 명령문(\"~해줘\", \"~그려줘\")이나 대화체 표현을 완전히 제거\n",
    "                   - 모든 설명은 영어로 변환\n",
    "                   - 묘사적이고 구체적인 명사구/형용사구 사용\n",
    "            \n",
    "                2. {image_model} 최적화 전략:\n",
    "                   - nova-canvas 최적화:\n",
    "                     * 구체적이고 정확한 시각적 설명\n",
    "                     * 해상도, 렌더링 품질 관련 키워드 추가\n",
    "                     * 세부 묘사를 중심으로 구성\n",
    "            \n",
    "                3. 프롬프트 구성 원칙:\n",
    "                   - 중요 요소를 문장 앞쪽에 배치\n",
    "                   - 콤마(,)로 구분하여 요소 간 가중치 균형 유지\n",
    "                   - 핵심 시각적 요소에 대한 디테일 강화\n",
    "                   - 부정 표현(\"no\", \"not\", \"without\" 등)은 사용하지 말고 negative_prompt 필드에 배치\n",
    "            \n",
    "                4. 프롬프트 길이는 1024자 이내로 유지하세요.\n",
    "                </instruction>\n",
    "            \n",
    "                <output_format>\n",
    "                DO NOT include any text or json symbol (```json```)outside the JSON format in the response\n",
    "                다음 형식의 JSON으로만 응답하세요:\n",
    "                {{\n",
    "                    \"image_prompt\": \n",
    "                    {{\n",
    "                        \"main_prompt\": \"재구성된 이미지 캡션 형태의 프롬프트\",\n",
    "                        \"negative_prompt\": \"제외할 요소들\"\n",
    "                    }}\n",
    "                }}\n",
    "                </output_format>\n",
    "                '''\n",
    "            )\n",
    "            \n",
    "            context = {\"image_model\": image_model}\n",
    "            system_prompts = system_prompts.format(**context)\n",
    "            system_prompts = bedrock_utils.get_system_prompt(system_prompts=system_prompts)\n",
    "            \n",
    "            user_prompts = dedent(\n",
    "                '''\n",
    "                Here is user's ask: <ask>{ask}</ask>\n",
    "                Here is extracted components: <subject>{subject}</subject>,\\n<action>{action}</action>,\\n<environment>{environment}</environment>\\n<lighting>{lighting}</lighting>\\n<style>{style}</style>\\n<camera_position>{camera_position}</camera_position>\n",
    "                '''\n",
    "            )\n",
    "            context = {\n",
    "                \"ask\": ask_repo,\n",
    "                \"subject\": prompt_components[\"subject\"],\n",
    "                \"action\": prompt_components[\"action\"],\n",
    "                \"environment\": prompt_components[\"environment\"],\n",
    "                \"lighting\": prompt_components[\"lighting\"],\n",
    "                \"style\": prompt_components[\"style\"],\n",
    "                \"camera_position\": prompt_components[\"camera_position\"]\n",
    "            }\n",
    "            user_prompts = user_prompts.format(**context)\n",
    "            \n",
    "            message = self._get_message_from_string(role=\"user\", string=user_prompts)            \n",
    "            self.messages.append(message)\n",
    "            messages.append(message)\n",
    "\n",
    "            resp, ai_message = self.llm_caller.invoke(messages=messages, system_prompts=system_prompts)\n",
    "            self.messages.append(ai_message)\n",
    "\n",
    "            results = eval(resp['text'])\n",
    "            image_prompt = results[\"image_prompt\"]\n",
    "\n",
    "            return self.state(image_prompt=image_prompt, prev_node=\"PROMPT_GENERATION_FOR_IMAGE\")\n",
    "\n",
    "        def image_generation(state):\n",
    "            \"\"\"Generate an image based on the prepared prompts.\n",
    "            \n",
    "            Args:\n",
    "                state: Current state dictionary.\n",
    "                \n",
    "            Returns:\n",
    "                Updated state with generated image path.\n",
    "            \"\"\"\n",
    "            print(\"---IMAGE GENERATION---\")\n",
    "            image_prompt = state[\"image_prompt\"]\n",
    "            generated_img_path = state.get(\"generated_img_path\", None)\n",
    "            task_type = state.get(\"task_type\", \"TEXT_IMAGE\")  # 추가: 기본값은 \"TEXT_IMAGE\"\n",
    "            mask_image = state.get(\"mask_image\", None)  # 추가: 마스크 이미지 파일 경로\n",
    "            original_image = state.get(\"original_image\", None)  # 추가: 마스크 이미지 파일 경로\n",
    "            print(\"generated_img_path\", generated_img_path)\n",
    "            print(\"task_type\", task_type)\n",
    "            print(\"mask_image\", mask_image)\n",
    "            print(\"original_image\", original_image)\n",
    "        \n",
    "            # 이미지 생성 요청 본문 생성\n",
    "            body = self._body_generator(\n",
    "                image_prompt,\n",
    "                taskType=task_type,\n",
    "                maskImage=mask_image,\n",
    "                original_image=original_image, ## in/out painting 위해서\n",
    "            )\n",
    "            \n",
    "            # 이미지 생성 API 호출\n",
    "            response = self.image_generation_model.bedrock_client.invoke_model(\n",
    "                body=body,\n",
    "                modelId=self.image_generation_model.model_id\n",
    "            )\n",
    "            response_body = json.loads(response.get(\"body\").read())\n",
    "            base64_image = response_body.get(\"images\")[0]\n",
    "            generated_img_path = self.show_save_image(base64_image)\n",
    "                            \n",
    "            return self.state(generated_img_path=generated_img_path, prev_node=\"IMAGE_GENERATION\")\n",
    "\n",
    "        def should_image_regeneration(state):\n",
    "            \"\"\"Determine if image should be regenerated.\n",
    "            \n",
    "            Args:\n",
    "                state: Current state dictionary.\n",
    "                \n",
    "            Returns:\n",
    "                str: Decision on whether to regenerate the image.\n",
    "            \"\"\"\n",
    "            print(\"---IMAGE CHECKER---\")\n",
    "            retouch, retry_count = state[\"retouch\"], state[\"retry_count\"]\n",
    "            \n",
    "            if retry_count <= 2 and retouch == \"true\":\n",
    "                print (\"---[REFLECTION] GO TO IMAGE REGENERATION---\")\n",
    "                print (\"retry_count: \", retry_count)\n",
    "                return \"regeneration\"\n",
    "            else:\n",
    "                print (\"---GO TO SHOW UP---\")\n",
    "                return \"continue\"\n",
    "            \n",
    "        # langgraph.graph에서 StateGraph와 END를 가져옵니다.\n",
    "        workflow = StateGraph(self.state)\n",
    "\n",
    "        # Todo 를 작성합니다.\n",
    "        workflow.add_node(\"ask_reformulation\", ask_reformulation)  # 이미지 생성을 위해 필요한 요소들이 준비되었는지 확인합니다.\n",
    "        workflow.add_node(\"check_readness_prompt_generation\", check_readness_prompt_generation)  # 이미지 생성을 위해 필요한 요소들이 준비되었는지 확인합니다.\n",
    "        workflow.add_node(\"prompt_generation_for_image\", prompt_generation_for_image)  # 요청을 이미지 생성용 프롬프트로 수정하는 노드를 추가합니다.\n",
    "        workflow.add_node(\"image_generation\", image_generation)  # 이미지 생성하는 노드를 추가합니다.\n",
    "        \n",
    "        workflow.add_edge(\"ask_reformulation\", \"check_readness_prompt_generation\")\n",
    "        workflow.add_edge(\"check_readness_prompt_generation\", \"prompt_generation_for_image\")\n",
    "        workflow.add_edge(\"prompt_generation_for_image\", \"image_generation\")\n",
    "        workflow.add_edge(\"image_generation\", END)\n",
    "        \n",
    "        # 시작점을 설정합니다.\n",
    "        workflow.set_entry_point(\"ask_reformulation\")\n",
    "\n",
    "        # 기록을 위한 메모리 저장소를 설정합니다.\n",
    "        memory = MemorySaver()\n",
    "\n",
    "        # 그래프를 컴파일합니다.\n",
    "        self.app = workflow.compile(checkpointer=memory)        \n",
    "        self.config = RunnableConfig(recursion_limit=100, configurable={\"thread_id\": \"Text2Image\"})\n",
    "\n",
    "    def invoke(self, **kwargs):\n",
    "        \"\"\"Run the image generation workflow with the given inputs.\n",
    "        \n",
    "        Args:\n",
    "            **kwargs: Input parameters including ask, image_model, \n",
    "                      and optional task_type, mask_image, and original_image.\n",
    "                      \n",
    "        Returns:\n",
    "            None: Results are stored in class attributes.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 새로운 매개변수 추가: task_type, mask_image\n",
    "        inputs = self.state(\n",
    "            ask=kwargs[\"ask\"], \n",
    "            image_model=kwargs[\"image_model\"],\n",
    "            task_type=kwargs.get(\"task_type\", \"TEXT_IMAGE\"),  # 기본값은 \"TEXT_IMAGE\"\n",
    "            mask_image=kwargs.get(\"mask_image\", None),  # 기본값은 None\n",
    "            original_image=kwargs.get(\"original_image\", None),  # 기본값은 None\n",
    "        )\n",
    "        self.file_name = kwargs.get('file_name', './generated_imgs/GENERATED_IMAGE.png')\n",
    "\n",
    "        # app.stream을 통해 입력된 메시지에 대한 출력을 스트리밍합니다.\n",
    "        for output in self.app.stream(inputs, self.config):\n",
    "            # 출력된 결과에서 키와 값을 순회합니다.\n",
    "            for key, value in output.items():\n",
    "                # 노드의 이름과 해당 노드에서 나온 출력을 출력합니다.\n",
    "                pprint.pprint(f\"\\nOutput from node '{key}':\")\n",
    "                pprint.pprint(\"---\")\n",
    "                # 출력 값을 예쁘게 출력합니다.\n",
    "                pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "                \n",
    "            # 각 출력 사이에 구분선을 추가합니다.\n",
    "            pprint.pprint(\"\\n---\\n\")\n",
    "            \n",
    "    \n",
    "    def show_graph(self, ):\n",
    "        \"\"\"Display a visual representation of the workflow graph.\n",
    "        \n",
    "        Returns:\n",
    "            None: Graph is displayed inline if possible.\n",
    "        \"\"\"\n",
    "        from IPython.display import Image, display\n",
    "\n",
    "        try:\n",
    "            display(\n",
    "                Image(self.app.get_graph(xray=True).draw_mermaid_png())\n",
    "            )  # 실행 가능한 객체의 그래프를 mermaid 형식의 PNG로 그려서 표시합니다. \n",
    "            # xray=True는 추가적인 세부 정보를 포함합니다.\n",
    "        except:\n",
    "            # 이 부분은 추가적인 의존성이 필요하며 선택적으로 실행됩니다.\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# Test 1 (outpainting) - 잘 안되는 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = GenAIOutPainting(\n",
    "    llm=llm,\n",
    "    image_generation_model=image_generation_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.invoke(\n",
    "    ask=dedent(\"리조또 한 접시\"),\n",
    "    image_model=\"nova-canvas\",\n",
    "    file_name=\"generated_imgs/GENERATED_IMAGE_RISOTTO_outpainting.png\",\n",
    "    mask_image=\"generated_imgs/GENERATED_IMAGE_PASTA_MASK.png\",\n",
    "    task_type=\"OUTPAINTING\",\n",
    "    original_image=\"generated_imgs/GENERATED_IMAGE_PASTA.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# Test 2 (outpainting) - 잘 되는 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = GenAIOutPainting(\n",
    "    llm=llm,\n",
    "    image_generation_model=image_generation_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.invoke(\n",
    "    ask=dedent(\"스타일리쉬한 식탁\"),\n",
    "    image_model=\"nova-canvas\",\n",
    "    file_name=\"generated_imgs/GENERATED_IMAGE_RISOTTO_outpainting_good_ex.png\",\n",
    "    mask_image=\"generated_imgs/GENERATED_IMAGE_PIZZA_MASK.png\",\n",
    "    task_type=\"OUTPAINTING\",\n",
    "    original_image=\"generated_imgs/GENERATED_IMAGE_PASTA.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지유님 통해서 정확한 mask 받으면 훨씬 잘 될듯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "3b41de70bedc0e302a3aeb58a0c77b854f2e56c8930e61a4aaa3340c96b01f1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
