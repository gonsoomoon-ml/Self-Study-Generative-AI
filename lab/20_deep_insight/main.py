
"""
Entry point script for the Strands Agent Demo.
"""
import os
import sys
import shutil
import asyncio
import argparse
import atexit
import signal
import time
import subprocess
from dotenv import load_dotenv
from src.utils.strands_sdk_utils import strands_utils
from src.graph.builder import build_graph

# Load environment variables
load_dotenv()

# Observability
from opentelemetry import trace, context
from src.utils.agentcore_observability import set_session_context, add_span_event

# Import event queue for unified event processing
from src.utils.event_queue import clear_queue

# Import Fargate session manager for cleanup
from src.tools.global_fargate_coordinator import get_global_session 

def remove_artifact_folder(folder_path="./artifacts/"):
    """
    ./artifact/ í´ë”ê°€ ì¡´ì¬í•˜ë©´ ì‚­ì œí•˜ëŠ” í•¨ìˆ˜

    Args:
        folder_path (str): ì‚­ì œí•  í´ë” ê²½ë¡œ
    """
    if os.path.exists(folder_path):
        print(f"'{folder_path}' í´ë”ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤...")
        try:
            shutil.rmtree(folder_path)
            print(f"'{folder_path}' í´ë”ê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e: print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
    else:
        print(f"'{folder_path}' í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

def cleanup_fargate_session():
    """Enhanced Fargate session cleanup with guaranteed task termination"""
    try:
        # 1. ì •ìƒì ì¸ ì„¸ì…˜ ì •ë¦¬ ì‹œë„
        fargate_manager = get_global_session()
        if fargate_manager and fargate_manager._session_manager and fargate_manager._session_manager.current_session:
            print("\nğŸ§¹ Starting final Fargate session cleanup...", flush=True)

            # ì„¸ì…˜ ì™„ë£Œ ì‹ í˜¸ ì „ì†¡ + S3 ì—…ë¡œë“œ ì™„ë£Œ ëŒ€ê¸°
            print("ğŸ“¤ Initiating final S3 upload and waiting for completion...", flush=True)
            completion_result = fargate_manager._session_manager.complete_session(wait_for_s3=True)

            if completion_result and completion_result.get("status") == "success":
                print("âœ… S3 upload confirmed - all Fargate artifacts uploaded", flush=True)
            else:
                print("âš ï¸ S3 upload status unclear, but proceeding with cleanup", flush=True)

        # 2. ê°•ì œë¡œ ëª¨ë“  Fargate íƒœìŠ¤í¬ ì •ë¦¬ (fail-safe)
        print("ğŸ” Checking for any remaining Fargate tasks...", flush=True)
        try:
            result = subprocess.run([
                'aws', 'ecs', 'list-tasks',
                '--cluster', 'my-fargate-cluster',
                '--query', 'taskArns[*]',
                '--output', 'text'
            ], capture_output=True, text=True, timeout=30)

            if result.returncode == 0 and result.stdout.strip():
                task_arns = result.stdout.strip().split('\t')
                task_ids = [arn.split('/')[-1] for arn in task_arns if arn.strip()]

                if task_ids:
                    print(f"ğŸ›‘ Found {len(task_ids)} running tasks, terminating...", flush=True)
                    for task_id in task_ids:
                        subprocess.run([
                            'aws', 'ecs', 'stop-task',
                            '--cluster', 'my-fargate-cluster',
                            '--task', task_id
                        ], capture_output=True, timeout=20)
                        print(f"   â€¢ Stopped task: {task_id[:12]}...", flush=True)
                    print("âœ… All orphaned Fargate tasks terminated", flush=True)
                else:
                    print("âœ… No running Fargate tasks found", flush=True)
            else:
                print("â„¹ï¸ Could not list Fargate tasks (cluster may not exist)", flush=True)

        except subprocess.TimeoutExpired:
            print("âš ï¸ Timeout while checking Fargate tasks", flush=True)
        except Exception as cleanup_error:
            print(f"âš ï¸ Error during task cleanup: {cleanup_error}", flush=True)

        print("âœ… Fargate session cleanup completed", flush=True)
    except Exception as e:
        print(f"âš ï¸ Error during Fargate session cleanup: {e}", flush=True)

def signal_handler(signum, frame):
    """Enhanced signal handler with immediate cleanup"""
    print(f"\nğŸ›‘ Received signal {signum}, initiating graceful shutdown...", flush=True)

    # ì¦‰ì‹œ cleanup ì‹¤í–‰ (atexitê³¼ ë³„ë„ë¡œ)
    cleanup_fargate_session()

    # ì§§ì€ ëŒ€ê¸° í›„ ê°•ì œ ì¢…ë£Œ
    time.sleep(2)
    print("ğŸ”š Process terminating...", flush=True)
    sys.exit(0)

def _setup_execution():
    """Initialize execution environment"""
    remove_artifact_folder()
    clear_queue()

    # Register cleanup handlers
    atexit.register(cleanup_fargate_session)
    signal.signal(signal.SIGINT, signal_handler)   # Ctrl+C
    signal.signal(signal.SIGTERM, signal_handler)  # Termination

    print("\n=== Starting Queue-Only Event Stream ===")
    print("ğŸ”§ Fargate session cleanup handlers registered")


def _print_conversation_history():
    """Print final conversation history"""
    print("\n=== Conversation History ===")
    from src.graph.nodes import _global_node_states
    shared_state = _global_node_states.get('shared', {})
    history = shared_state.get('history', [])

    if history:
        for hist_item in history:
            print(f"[{hist_item['agent']}] {hist_item['message']}")
    else:
        print("No conversation history found")

async def graph_streaming_execution(payload):
    """Execute full graph streaming workflow using new graph.stream_async method"""

    _setup_execution()

    # Get user query from payload
    user_query = payload.get("user_query", "")
    context_token = set_session_context("default")

    try:
        # Get tracer for main application
        tracer = trace.get_tracer(
            instrumenting_module_name=os.getenv("TRACER_MODULE_NAME", "insight_extractor_agent"),
            instrumenting_library_version=os.getenv("TRACER_LIBRARY_VERSION", "1.0.0")
        )
        with tracer.start_as_current_span("insight_extractor_session") as span:   
            
            # Build graph and use stream_async method
            graph = build_graph()
            
            #########################
            ## modification START  ##
            #########################

            # Stream events from graph execution
            graph_input = {
                "request": user_query,
                "request_prompt": f"Here is a user request: <user_request>{user_query}</user_request>"
            }

            # CSV íŒŒì¼ ê²½ë¡œê°€ ìˆìœ¼ë©´ ì¶”ê°€
            csv_file_path = payload.get("csv_file_path")
            if csv_file_path:
                graph_input["csv_file_path"] = csv_file_path

            async for event in graph.stream_async(graph_input):
                yield event

            #########################
            ## modification END    ##
            #########################
            
            _print_conversation_history()
            print("=== Queue-Only Event Stream Complete ===")

            # Add Event
            add_span_event(span, "user_query", {"user-query": str(user_query)}) 

    finally:
        context.detach(context_token)

if __name__ == "__main__":
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Strands Agent Demo')
    parser.add_argument('--user_query', type=str, help='User query for the agent')

    args, unknown = parser.parse_known_args()


    #########################
    ## modification START  ##
    #########################

    # Use argparse values if provided, otherwise use predefined values
    if args.user_query:
        payload = {
            "user_query": args.user_query,
            "csv_file_path": "./data/test-dynamic-data.csv"  # ë™ì  CSV íŒŒì¼ ê²½ë¡œ (í…ŒìŠ¤íŠ¸ìš©)
        }
    else:
        # Simple test version:
        payload = {
            "user_query": "'./data/Dat-fresh-food-claude.csv'ë¥¼ ì½ì–´ì„œ ì´ ë§¤ì¶œì•¡ ê³„ì‚°í•˜ê³  ê°„ë‹¨í•œ ì°¨íŠ¸ í•˜ë‚˜ë§Œ ê·¸ë ¤ì„œ pdfë¡œ ì €ì¥í•´ì¤˜",
            "csv_file_path": "./data/Dat-fresh-food-claude.csv"  # ë™ì  CSV íŒŒì¼ ê²½ë¡œ
        }

    #########################
    ## modification END    ##
    #########################

    remove_artifact_folder()

    # Use full graph streaming execution for real-time streaming with graph structure
    async def run_streaming():
        async for event in graph_streaming_execution(payload):
            strands_utils.process_event_for_display(event)

    asyncio.run(run_streaming())

