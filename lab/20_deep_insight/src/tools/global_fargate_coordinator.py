#!/usr/bin/env python3
"""
Global Fargate Session Manager for Multi-Agent Workflows

This module provides a singleton session manager that coordinates Fargate container
sessions across multiple tools and concurrent requests in a multi-agent data analysis
environment.

Architecture:
    - Singleton Pattern: Single global instance manages all sessions
    - Session Isolation: Each request gets its own HTTP client, cookies, and container
    - Container Lifecycle: Automated creation, health checks, and cleanup
    - ALB Integration: Sticky session management with subprocess-based cookie acquisition
    - S3 Integration: Automatic data synchronization for CSV files
    - Error Handling: Exponential backoff retry with fail-fast for configuration errors

Key Features:
    1. Multi-Request Session Isolation
       - Separate HTTP clients per request (cookie isolation)
       - IP-based container ownership tracking
       - Prevents session conflicts in concurrent workflows

    2. Container Lifecycle Management
       - Fargate task creation with ECS
       - ALB target registration and health checks
       - Automatic cleanup on workflow completion
       - Orphaned container detection and cleanup

    3. ALB Sticky Session Management
       - Subprocess-based cookie acquisition (process isolation)
       - Round-robin retry logic for target IP matching
       - Session ID validation for multi-job support

    4. Robust Error Handling
       - Exponential backoff for transient errors (3^attempt)
       - Fail-fast for configuration errors (IAM, VPC, etc.)
       - Per-request failure tracking with limits

    5. S3 Data Synchronization
       - CSV file upload with session ID prefixes
       - Container file sync via HTTP API
       - Automatic cleanup on session completion

Usage Example:
    ```python
    # Get singleton instance
    session_mgr = get_global_session()

    # Set request context (required)
    session_mgr.set_request_context("request-123")

    # Create session with data
    success = session_mgr.ensure_session_with_data("data.csv")

    # Execute code in container
    result = session_mgr.execute_code("import pandas as pd\\ndf = pd.read_csv('data.csv')")

    # Cleanup when done
    session_mgr.cleanup_session()
    ```

Environment Variables Required:
    - AWS_REGION: AWS region for service calls
    - ECS_CLUSTER_NAME: ECS cluster for Fargate tasks
    - ALB_TARGET_GROUP_ARN: ALB target group ARN
    - S3_BUCKET_NAME: S3 bucket for data/results
    - TASK_DEFINITION_ARN: ECS task definition ARN
    - CONTAINER_NAME: Container name in task definition

Thread Safety:
    This module has MIXED thread safety guarantees:

    ‚úÖ THREAD-SAFE operations (safe for concurrent requests):
       - cleanup_session(request_id): Safe when explicit request_id is provided
         ‚Üí Each request cleans up its own isolated dictionary keys
         ‚Üí No shared state accessed during cleanup
         ‚Üí CPython GIL provides atomicity for dict operations

    ‚ùå NOT THREAD-SAFE operations (require sequential execution):
       - set_request_context(): Mutates shared _current_request_id
       - ensure_session(): Relies on shared _current_request_id
       - ensure_session_with_data(): Relies on shared _current_request_id

    Current AgentCore Integration:
       ‚úÖ Safe - Runtime always passes explicit request_id to cleanup_session()
       ‚ö†Ô∏è  Session creation requires sequential execution or process isolation

    Recommendation:
       Use separate process instances for concurrent requests, OR ensure
       set_request_context() ‚Üí ensure_session() ‚Üí cleanup_session(request_id)
       sequence runs sequentially per request.

Notes:
    - Automatic cleanup registered via atexit
    - Cookies are session-specific (AWSALB sticky sessions)
    - Health checks wait up to 150 seconds for container readiness
    - Cookie acquisition timeout: 240 seconds (4 minutes)
"""

# ============================================================================
# IMPORTS
# ============================================================================

import logging
import os
import time
import json
import subprocess
import atexit
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables (don't override Runtime env vars)
load_dotenv(override=False)

# ECS and ALB configuration from environment
# These must be provided via environment variables (no hardcoded defaults)
ECS_CLUSTER_NAME = os.getenv("ECS_CLUSTER_NAME")
ALB_TARGET_GROUP_ARN = os.getenv("ALB_TARGET_GROUP_ARN")
S3_BUCKET_NAME = os.getenv("S3_BUCKET_NAME")

# Third-party imports
import boto3
import requests
from botocore.exceptions import ClientError

# Local imports
from src.tools.fargate_container_controller import SessionBasedFargateManager

# ============================================================================
# LOGGER SETUP
# ============================================================================

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


# ============================================================================
# GLOBAL FARGATE SESSION MANAGER (SINGLETON)
# ============================================================================

class GlobalFargateSessionManager:
    """
    Singleton session manager for coordinating Fargate container sessions.

    Features:
    - Per-request session isolation (cookies, HTTP clients, containers)
    - Exponential backoff retry for session creation
    - ALB health checks and sticky session management
    - S3 data synchronization
    - Automatic cleanup on exit
    """

    # ========================================================================
    # CLASS VARIABLES (SINGLETON STATE)
    # ========================================================================

    _instance = None
    _session_manager = None
    _sessions = {}  # {request_id: session_info} - Per-request session management
    _http_clients = {}  # {request_id: http_session} - Per-request HTTP client (cookie isolation)
    _used_container_ips = {}  # {container_ip: request_id} - IP-based container ownership tracking
    _current_request_id = None  # Current context request ID
    _session_creation_failures = {}  # {request_id: failure_count} - Session creation failure tracking
    _cleaned_up_requests = set()  # Cleaned-up request IDs (prevents recreation)

    # ========================================================================
    # CONSTANTS (TIMEOUTS AND RETRY LIMITS)
    # ========================================================================

    # Session Creation
    SESSION_CREATION_MAX_RETRIES = 5  # Maximum session creation retry attempts
    EXPONENTIAL_BACKOFF_BASE = 3      # Base for exponential backoff (3^attempt)

    # Code Execution
    CODE_EXECUTION_MAX_RETRIES = 3    # Maximum code execution retry attempts
    CODE_EXECUTION_RETRY_DELAY = 2    # Delay between retries (seconds)

    # ALB Health Check Wait Times (seconds)
    ALB_INITIAL_WAIT_DURATION = 60    # Total wait before health checks start
    ALB_WAIT_ITERATIONS = 6           # Number of keep-alive log iterations
    ALB_WAIT_INTERVAL = 10            # Interval for each iteration (60s / 6 = 10s)

    # Container Health Check (seconds)
    HEALTH_CHECK_MAX_ATTEMPTS = 30    # Maximum health check attempts
    HEALTH_CHECK_INTERVAL = 5         # Interval between health checks

    # Timeouts (seconds)
    COOKIE_ACQUISITION_TIMEOUT = 240  # Cookie acquisition subprocess timeout (4 minutes)
    FILE_SYNC_TIMEOUT = 30            # File sync HTTP request timeout
    FILE_SYNC_WAIT = 10               # Wait after file sync for completion

    # ========================================================================
    # üì¶ INITIALIZATION (SINGLETON PATTERN)
    # ========================================================================

    def __new__(cls):
        """Singleton pattern implementation"""
        if cls._instance is None:
            cls._instance = super(GlobalFargateSessionManager, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        """Initialize the session manager (only once due to singleton)"""
        if self._session_manager is None:
            logger.info("üöÄ Initializing Global Fargate Session Manager")
            self._session_manager = SessionBasedFargateManager()
            atexit.register(self._auto_cleanup)

    # ========================================================================
    # üåê PUBLIC API METHODS
    # ========================================================================

    def set_request_context(self, request_id: str):
        """Set current request context"""
        self._current_request_id = request_id
        logger.info(f"üìã Request context set: {request_id}")

    def ensure_session(self):
        """
        Ensure session exists or create new one (with exponential backoff retry)

        Returns:
            bool: True if session exists or was created successfully, False otherwise
        """
        try:
            if not self._current_request_id:
                raise Exception("Request context not set. Call set_request_context() first.")

            # Prevent new session creation for already cleaned-up requests
            if self._current_request_id in self._cleaned_up_requests:
                error_msg = f"‚ùå FATAL: Request {self._current_request_id} already cleaned up - cannot create new session. This prevents duplicate container creation after workflow completion."
                logger.error(error_msg)
                raise Exception(error_msg)

            # Check if session exists for current request
            if self._current_request_id in self._sessions:
                return self._reuse_existing_session()

            # Create new session with exponential backoff
            return self._create_new_session()

        except Exception as e:
            logger.error(f"‚ùå Failed to ensure session: {e}")

            # Re-raise fatal errors (stop workflow)
            if "FATAL" in str(e):
                raise

            return False

    def ensure_session_with_data(self, csv_file_path: str):
        """
        Create session with CSV data (session creation ‚Üí S3 upload ‚Üí container sync)

        Args:
            csv_file_path: Path to CSV file to upload

        Returns:
            bool: True if successful, False otherwise
        """
        try:
            logger.info(f"üöÄ Creating session with data: {csv_file_path}")

            # 1. Create session first (generates timestamp)
            if not self.ensure_session():
                raise Exception("Failed to create Fargate session")

            # 2. Upload to S3 using generated session ID
            session_id = self._sessions[self._current_request_id]['session_id']
            s3_key = self._upload_csv_to_s3_with_session_id(csv_file_path, session_id)
            logger.info(f"üì§ CSV uploaded to S3: {s3_key}")

            # 3. Sync S3 ‚Üí container local storage
            self._sync_csv_from_s3_to_container(s3_key)
            logger.info("‚úÖ CSV file synced to container")

            return True

        except Exception as e:
            logger.error(f"‚ùå Failed to create session with data: {e}")
            return False

    def execute_code(self, code: str, description: str = ""):
        """
        Execute code with automatic session management and connection retry

        Args:
            code: Python code to execute
            description: Description of the code execution

        Returns:
            dict: Execution result or error
        """
        for attempt in range(1, self.CODE_EXECUTION_MAX_RETRIES + 1):
            try:
                # Ensure session exists
                if not self.ensure_session():
                    return {"error": "Failed to create or maintain session"}

                # Execute code
                result = self._session_manager.execute_code(code, description)

                # Return immediately on success
                return result

            except Exception as e:
                error_msg = str(e)

                # Check if it's a connection error
                is_connection_error = any(keyword in error_msg.upper() for keyword in [
                    "CONNECTION FAILED",
                    "NOT RESPONDING",
                    "TIMEOUT",
                    "CONNECTIONERROR",
                    "HTTPERROR"
                ])

                if is_connection_error:
                    # Connection error - retry
                    logger.warning(f"‚ö†Ô∏è Connection error (attempt {attempt}/{self.CODE_EXECUTION_MAX_RETRIES}): {error_msg}")

                    if attempt < self.CODE_EXECUTION_MAX_RETRIES:
                        logger.info(f"üîÑ Retrying in {self.CODE_EXECUTION_RETRY_DELAY} seconds...")
                        time.sleep(self.CODE_EXECUTION_RETRY_DELAY)
                    else:
                        logger.error(f"‚ùå Connection failed after {self.CODE_EXECUTION_MAX_RETRIES} attempts. Giving up.")
                        return {
                            "error": f"Connection failed after {self.CODE_EXECUTION_MAX_RETRIES} attempts: {error_msg}"
                        }
                else:
                    # Code execution error - don't retry
                    logger.error(f"‚ùå Code execution failed: {e}")
                    # NOTE: Don't reset session to None!
                    # Keep session alive so next agent can retry
                    # Multiple agents share the same container, so don't reset session
                    return {"error": str(e)}

    def cleanup_session(self, request_id: str = None):
        """
        Clean up session for specific request

        Args:
            request_id: Request ID to cleanup (defaults to current request)
        """
        try:
            # Use current context if request_id not provided
            cleanup_request_id = request_id or self._current_request_id

            if not cleanup_request_id:
                logger.warning("‚ö†Ô∏è No request ID for cleanup")
                return

            if cleanup_request_id in self._sessions:
                session_info = self._sessions[cleanup_request_id]
                logger.info(f"üßπ Cleaning up session for request {cleanup_request_id}: {session_info['session_id']}")

                container_ip = session_info.get('container_ip')

                # FIX: Call complete_session() first (before ALB removal)
                # 1. Allow container to upload to S3 first
                logger.info(f"üèÅ Completing session (S3 upload)...")
                self._session_manager.current_session = session_info['fargate_session']
                self._session_manager.complete_session()

                # 2. Then release container IP and remove from ALB (safe now)
                if container_ip and container_ip in self._used_container_ips:
                    del self._used_container_ips[container_ip]
                    logger.info(f"üßπ Released container IP: {container_ip}")
                    logger.info(f"   Remaining IPs: {list(self._used_container_ips.keys())}")

                    # Remove container from ALB Target Group (prevents zombie targets)
                    # Execute after complete_session() to prevent HTTP 502 errors
                    self._deregister_from_alb(container_ip)

                # Remove from session dictionary
                del self._sessions[cleanup_request_id]
                logger.info(f"‚úÖ Session cleanup completed. Remaining sessions: {len(self._sessions)}")
            else:
                logger.warning(f"‚ö†Ô∏è No session found for request {cleanup_request_id}")

            # Clean up HTTP client (remove cookies)
            if cleanup_request_id in self._http_clients:
                del self._http_clients[cleanup_request_id]
                logger.info(f"üç™ Removed HTTP client for request {cleanup_request_id}")

            # Clean up failure counter
            if cleanup_request_id in self._session_creation_failures:
                del self._session_creation_failures[cleanup_request_id]
                logger.info(f"üßπ Cleared failure counter for request {cleanup_request_id}")

            # Track cleaned-up request ID (prevent recreation)
            self._cleaned_up_requests.add(cleanup_request_id)
            logger.info(f"üîí Request {cleanup_request_id} marked as cleaned up - new session creation blocked")

        except Exception as e:
            logger.error(f"‚ùå Session cleanup failed: {e}")

    # ========================================================================
    # üîß SESSION MANAGEMENT (PRIVATE HELPERS)
    # ========================================================================

    def _get_aws_region(self) -> str:
        """
        Get AWS region from environment with validation

        Returns:
            str: AWS region name

        Raises:
            ValueError: If AWS_REGION environment variable is not set
        """
        aws_region = os.getenv('AWS_REGION')
        if not aws_region:
            raise ValueError("AWS_REGION environment variable is required but not set")
        return aws_region

    def _cleanup_failed_session(self):
        """Clean up session state after creation failure"""
        if self._current_request_id in self._sessions:
            del self._sessions[self._current_request_id]
        self._cleanup_orphaned_containers()

    def _increment_failure_counter(self):
        """Increment session creation failure counter for current request"""
        failure_count = self._session_creation_failures.get(self._current_request_id, 0)
        self._session_creation_failures[self._current_request_id] = failure_count + 1

    def _log_active_sessions(self, attempt: int):
        """Log information about currently active sessions"""
        active_sessions = [req_id for req_id in self._sessions.keys() if req_id not in self._cleaned_up_requests]
        logger.info(f"üì¶ Creating new Fargate session for request {self._current_request_id} (attempt {attempt}/{self.SESSION_CREATION_MAX_RETRIES})...")
        logger.info(f"   Current active sessions: {len(active_sessions)}")
        if active_sessions:
            logger.info(f"   Active request IDs: {active_sessions}")
            logger.info(f"   Active container IPs: {[self._sessions[req_id]['container_ip'] for req_id in active_sessions if req_id in self._sessions]}")

    def _create_fargate_container(self):
        """Create and configure Fargate container with HTTP session"""
        timestamp_id = datetime.now().strftime("%Y-%m-%d-%H-%M-%S")

        fargate_session_info = self._session_manager.create_session(
            session_id=timestamp_id,
            max_executions=300
        )

        # Inject HTTP session (per-request cookie isolation)
        http_client = self._get_http_client(self._current_request_id)
        self._session_manager.set_http_session(http_client)
        logger.info(f"üîó HTTP session injected for request {self._current_request_id}")

        return fargate_session_info

    def _register_container_ip(self, private_ip: str):
        """Register container IP for current request"""
        self._used_container_ips[private_ip] = self._current_request_id
        logger.info(f"üìù Registered container IP: {private_ip}")
        logger.info(f"   Request ID: {self._current_request_id}")
        logger.info(f"   All registered IPs: {list(self._used_container_ips.keys())}")

    def _save_session(self, fargate_session_info: dict, container_ip: str):
        """Save session information after successful creation"""
        self._sessions[self._current_request_id] = {
            'session_id': fargate_session_info['session_id'],
            'request_id': self._current_request_id,
            'container_ip': container_ip,
            'fargate_session': self._session_manager.current_session,
            'created_at': datetime.now()
        }
        logger.info(f"‚úÖ Session created and saved for request {self._current_request_id}: {fargate_session_info['session_id']}")
        logger.info(f"   Total active sessions: {len(self._sessions)}")

        # Session creation success - reset failure counter
        if self._current_request_id in self._session_creation_failures:
            del self._session_creation_failures[self._current_request_id]

    def _handle_aws_session_error(self, error_code: str, error_message: str, attempt: int) -> bool:
        """
        Handle AWS ClientError during session creation

        Returns:
            True to continue retry loop, False to stop (error was raised)
        """
        # Configuration errors - FAIL FAST (don't retry)
        NON_RETRYABLE_ERRORS = [
            'ValidationException',
            'InvalidParameterException',
            'AccessDeniedException',
            'ResourceNotFoundException',
            'UnauthorizedException',
        ]

        if error_code in NON_RETRYABLE_ERRORS:
            logger.error(f"‚ùå FATAL: Non-retryable configuration error detected: {error_code}")
            logger.error(f"   Error: {error_message}")
            logger.error(f"   Fix the configuration and try again. Not retrying.")
            self._increment_failure_counter()
            raise

        # Transient errors - retry with exponential backoff
        if attempt < self.SESSION_CREATION_MAX_RETRIES:
            wait_time = self.EXPONENTIAL_BACKOFF_BASE ** attempt
            logger.warning(f"‚è≥ Transient error - waiting {wait_time}s before retry (exponential backoff: {self.EXPONENTIAL_BACKOFF_BASE}^{attempt})...")
            time.sleep(wait_time)
            return True
        else:
            # Last attempt failed
            self._increment_failure_counter()
            logger.error(f"‚ùå FATAL: Session creation failed {self.SESSION_CREATION_MAX_RETRIES} times for request {self._current_request_id}")
            logger.error(f"   Total backoff time: {sum(self.EXPONENTIAL_BACKOFF_BASE ** i for i in range(1, self.SESSION_CREATION_MAX_RETRIES + 1))} seconds")
            raise

    def _handle_generic_session_error(self, attempt: int) -> bool:
        """
        Handle generic exceptions during session creation

        Returns:
            True to continue retry loop, False to stop (error was raised)
        """
        if attempt < self.SESSION_CREATION_MAX_RETRIES:
            wait_time = self.EXPONENTIAL_BACKOFF_BASE ** attempt
            logger.warning(f"‚è≥ Waiting {wait_time}s before retry (exponential backoff: {self.EXPONENTIAL_BACKOFF_BASE}^{attempt})...")
            time.sleep(wait_time)
            return True
        else:
            # Last attempt failed
            self._increment_failure_counter()
            logger.error(f"‚ùå FATAL: Session creation failed {self.SESSION_CREATION_MAX_RETRIES} times for request {self._current_request_id}")
            logger.error(f"   Total backoff time: {sum(self.EXPONENTIAL_BACKOFF_BASE ** i for i in range(1, self.SESSION_CREATION_MAX_RETRIES + 1))} seconds")
            raise

    def _reuse_existing_session(self):
        """Reuse existing session (with health check)"""
        session_info = self._sessions[self._current_request_id]
        container_ip = session_info.get('container_ip', 'unknown')

        logger.info(f"‚ôªÔ∏è Reusing existing session for request {self._current_request_id}: {session_info['session_id']}")

        # üîç Container Health Check (ALB Target Health)
        if container_ip != 'unknown':
            target_health = self._check_alb_target_health(container_ip)
            logger.info(f"   üè• Container ALB Health: {target_health}")

            if target_health not in ['healthy', 'initial']:
                logger.warning(f"‚ö†Ô∏è WARNING: Reusing session with container in '{target_health}' state!")
                logger.warning(f"   This may cause connection failures!")
                logger.warning(f"   Container IP: {container_ip}")
                logger.warning(f"   Session ID: {session_info['session_id']}")
                logger.warning(f"   Consider implementing automatic cleanup for stopped containers")

        # Update SessionBasedFargateManager's current_session
        self._session_manager.current_session = session_info['fargate_session']

        # Re-inject HTTP session (required even when reusing session)
        http_client = self._get_http_client(self._current_request_id)
        self._session_manager.set_http_session(http_client)

        return True

    def _create_new_session(self):
        """Create new session (with exponential backoff retry)"""
        for attempt in range(1, self.SESSION_CREATION_MAX_RETRIES + 1):
            try:
                # Log concurrent execution detection
                self._log_active_sessions(attempt)

                # Create Fargate container and inject HTTP session
                fargate_session_info = self._create_fargate_container()

                # Register container IP
                expected_private_ip = self._session_manager.current_session['private_ip']
                self._register_container_ip(expected_private_ip)

                # Wait for ALB health check and acquire cookie
                if not self._wait_for_container_ready(expected_private_ip, fargate_session_info['session_id']):
                    # Release IP registration on failure
                    if expected_private_ip in self._used_container_ips:
                        del self._used_container_ips[expected_private_ip]
                    return False

                # Save session after health check + cookie acquisition
                self._save_session(fargate_session_info, expected_private_ip)

                return True

            except ClientError as create_error:
                error_code = create_error.response['Error']['Code']
                error_message = create_error.response['Error']['Message']
                logger.error(f"‚ùå Session creation failed (attempt {attempt}/{self.SESSION_CREATION_MAX_RETRIES}): [{error_code}] {error_message}")

                # Cleanup only if session creation itself failed
                self._cleanup_failed_session()

                # Handle AWS error (may raise exception)
                self._handle_aws_session_error(error_code, error_message, attempt)

            except Exception as create_error:
                # Handle non-AWS exceptions (e.g., network errors, Python exceptions)
                logger.error(f"‚ùå Session creation failed (attempt {attempt}/{self.SESSION_CREATION_MAX_RETRIES}): {create_error}")

                # Cleanup only if session creation itself failed
                self._cleanup_failed_session()

                # Handle generic error (may raise exception)
                self._handle_generic_session_error(attempt)

    def _wait_for_container_ready(self, expected_ip: str, session_id: str) -> bool:
        """Wait for container readiness (ALB Health Check + Cookie acquisition)"""
        # Wait for ALB to begin health checks (with keep-alive logging)
        logger.info(f"‚è≥ Waiting {self.ALB_INITIAL_WAIT_DURATION} seconds for ALB to begin health checks...")
        logger.info(f"   This prevents 'ALB never sent health checks' issue")

        # Keep-alive: Split 60s into 6 iterations of 10s with logging
        for wait_i in range(self.ALB_WAIT_ITERATIONS):
            time.sleep(self.ALB_WAIT_INTERVAL)
            logger.info(f"   ‚è±Ô∏è  Waiting for ALB... ({(wait_i+1)*self.ALB_WAIT_INTERVAL}/{self.ALB_INITIAL_WAIT_DURATION}s)")

        # Wait for ALB Health Check (until container becomes healthy)
        logger.info(f"‚è∞ Waiting for container {expected_ip} to be healthy in ALB...")
        alb_healthy = False
        max_wait_time = self.HEALTH_CHECK_MAX_ATTEMPTS * self.HEALTH_CHECK_INTERVAL
        for wait_attempt in range(1, self.HEALTH_CHECK_MAX_ATTEMPTS + 1):
            target_health = self._check_alb_target_health(expected_ip)
            logger.info(f"   Attempt {wait_attempt}/{self.HEALTH_CHECK_MAX_ATTEMPTS}: ALB health = {target_health}")

            if target_health == 'healthy':
                logger.info(f"‚úÖ Container is healthy in ALB after {wait_attempt * self.HEALTH_CHECK_INTERVAL}s")
                alb_healthy = True
                break
            elif target_health in ['unhealthy', 'draining']:
                logger.warning(f"‚ö†Ô∏è Container is {target_health} - continuing to wait...")
            elif target_health == 'not_registered':
                logger.info(f"   Container not yet registered to ALB - waiting...")

            if wait_attempt < self.HEALTH_CHECK_MAX_ATTEMPTS:
                time.sleep(self.HEALTH_CHECK_INTERVAL)

        if not alb_healthy:
            logger.warning(f"‚ö†Ô∏è Container not healthy after {max_wait_time}s, but will try cookie acquisition anyway")

        # Acquire IP-based cookie (with session ID validation)
        cookie_acquired = self._acquire_cookie_for_ip(expected_ip, session_id)

        if not cookie_acquired:
            logger.warning(f"‚ö†Ô∏è Failed to acquire Sticky Session cookie")
            logger.warning(f"   Releasing IP registration: {expected_ip}")
            logger.warning(f"   Cookie acquisition failed - session NOT saved")
            return False

        return True

    def _get_http_client(self, request_id: str):
        """Return HTTP client for request (cookie isolation)"""
        if request_id not in self._http_clients:
            self._http_clients[request_id] = requests.Session()
            logger.info(f"üç™ Created new HTTP client for request {request_id}")
        return self._http_clients[request_id]

    def _check_alb_target_health(self, target_ip: str) -> str:
        """
        Check if the specified IP is registered and healthy in the ALB target group

        Returns:
            'healthy', 'unhealthy', 'initial', 'draining', 'unused', 'not_registered', 'unknown'
        """
        try:
            if not ALB_TARGET_GROUP_ARN:
                raise ValueError("ALB_TARGET_GROUP_ARN environment variable is required")

            elbv2_client = boto3.client('elbv2', region_name=self._get_aws_region())
            response = elbv2_client.describe_target_health(TargetGroupArn=ALB_TARGET_GROUP_ARN)

            for target_health in response.get('TargetHealthDescriptions', []):
                if target_health['Target']['Id'] == target_ip:
                    state = target_health['TargetHealth']['State']
                    return state

            return 'not_registered'
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to check ALB target health: {e}")
            return 'unknown'

    # ========================================================================
    # üç™ COOKIE ACQUISITION (SUBPROCESS-BASED)
    # ========================================================================

    def _run_cookie_subprocess(self, script_path: Path, expected_ip: str, session_id: str) -> subprocess.CompletedProcess:
        """Run cookie acquisition subprocess and return result"""
        logger.info(f"üîß Launching subprocess for cookie acquisition...")
        logger.info(f"   Script: {script_path}")

        result = subprocess.run(
            ["python3", str(script_path), self._session_manager.alb_dns, expected_ip, session_id],
            capture_output=True,
            text=True,
            timeout=self.COOKIE_ACQUISITION_TIMEOUT
        )

        # Log subprocess stderr
        if result.stderr:
            for line in result.stderr.strip().split('\n'):
                if line.strip():
                    logger.info(f"   {line}")

        return result

    def _parse_cookie_result(self, result: subprocess.CompletedProcess) -> dict:
        """Parse cookie acquisition subprocess result"""
        output = result.stdout.strip()
        if not output:
            logger.error(f"‚ùå Subprocess produced no output")
            if result.stderr:
                logger.error(f"   Full stderr: {result.stderr}")
            return None

        try:
            data = json.loads(output)
            return data
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Failed to parse subprocess output: {e}")
            logger.error(f"   Output: {result.stdout}")
            if result.stderr:
                logger.error(f"   Full stderr: {result.stderr}")
            return None

    def _store_acquired_cookie(self, cookie_value: str, attempt: int, actual_ip: str) -> bool:
        """Store acquired cookie in HTTP client"""
        logger.info(f"‚úÖ Cookie acquired! (attempt {attempt})")
        logger.info(f"   My container IP: {actual_ip}")
        logger.info(f"   Cookie value: {cookie_value[:20]}...")

        # Set cookie in HTTP client (reuse existing session)
        http_client = self._get_http_client(self._current_request_id)
        http_client.cookies.set('AWSALB', cookie_value)

        return True

    def _acquire_cookie_for_ip(self, expected_ip: str, session_id: str) -> bool:
        """
        Acquire sticky session cookie from container with specific IP using subprocess

        Why use subprocess:
        - Complete process isolation ‚Üí Independent TCP Connection Pool
        - ALB Round Robin works correctly (no Connection: close/Pool Control needed)
        - Multi-job environment: Each job can acquire cookies independently

        How it works:
        1. Run inline script in isolated Python process
        2. Try 40 times inside subprocess (each with new TCP connection)
        3. Reach target container via ALB Round Robin and acquire cookie
        4. Validate session ID to confirm correct container (multi-job support)
        5. Return result as JSON (stdout)
        6. Parent process sets cookie in HTTP client
        """
        logger.info(f"üç™ Acquiring cookie for container: {expected_ip}")
        logger.info(f"   Session ID: {session_id}")
        other_ips = [ip for ip in self._used_container_ips.keys() if ip != expected_ip]
        if other_ips:
            logger.info(f"   Other active containers: {other_ips}")
        else:
            logger.info(f"   No other active containers")

        try:
            # Get path to cookie acquisition subprocess script
            current_file = Path(__file__)
            script_path = current_file.parent / "cookie_acquisition_subprocess.py"

            if not script_path.exists():
                raise FileNotFoundError(f"Cookie acquisition script not found: {script_path}")

            # Run subprocess
            result = self._run_cookie_subprocess(script_path, expected_ip, session_id)

            # Parse result
            data = self._parse_cookie_result(result)
            if not data:
                return False

            # Check success
            if data.get("success"):
                cookie_value = data.get("cookie")
                attempt = data.get("attempt")
                actual_ip = data.get("ip")
                return self._store_acquired_cookie(cookie_value, attempt, actual_ip)
            else:
                error_msg = data.get("error", "Unknown error")
                logger.error(f"‚ùå Cookie acquisition failed: {error_msg}")
                logger.error(f"   Expected IP: {expected_ip}")
                logger.error(f"   Registered IPs: {list(self._used_container_ips.keys())}")
                return False

        except subprocess.TimeoutExpired as e:
            logger.error(f"‚ùå Cookie acquisition timeout ({self.COOKIE_ACQUISITION_TIMEOUT} seconds)")
            logger.error(f"   Expected IP: {expected_ip}")
            if e.stderr:
                # Also log timeout stderr (safe bytes ‚Üí string conversion)
                stderr_text = str(e.stderr, 'utf-8') if isinstance(e.stderr, bytes) else e.stderr
                for line in stderr_text.strip().split('\n'):
                    if line.strip():
                        logger.error(f"   {line}")
            return False
        except Exception as e:
            logger.error(f"‚ùå Cookie acquisition subprocess failed: {e}")
            logger.error(f"   Expected IP: {expected_ip}")
            return False

    # ========================================================================
    # üì§ DATA SYNC METHODS (S3 UPLOAD/DOWNLOAD)
    # ========================================================================

    def _upload_csv_to_s3_with_session_id(self, csv_file_path: str, session_id: str) -> str:
        """Upload CSV file to S3 using existing session ID (prevents timestamp mismatch)"""
        try:
            # Use existing session ID (no new timestamp generation)
            original_filename = os.path.basename(csv_file_path)
            s3_key = f"deep-insight/fargate_sessions/{session_id}/input/{original_filename}"

            # S3 upload
            s3_client = boto3.client('s3', region_name=self._get_aws_region())
            s3_client.upload_file(
                csv_file_path,
                S3_BUCKET_NAME,
                s3_key,
                ExtraArgs={'ContentType': 'text/csv'}
            )

            logger.info(f"üì§ Uploaded {csv_file_path} ‚Üí s3://{S3_BUCKET_NAME}/{s3_key}")
            return s3_key

        except Exception as e:
            logger.error(f"‚ùå S3 upload failed: {e}")
            raise

    def _sync_csv_from_s3_to_container(self, s3_key: str):
        """Synchronize CSV file from S3 to container (Enhanced Logging)"""
        try:
            # ALB DNS (get from session manager)
            alb_dns = self._session_manager.alb_dns
            filename = s3_key.split('/')[-1]

            # ‚úÖ 1. Start log
            logger.info(f"üîÑ Starting file sync...")
            logger.info(f"   S3 Key: {s3_key}")
            logger.info(f"   Filename: {filename}")
            logger.info(f"   Target: /app/data/{filename}")

            # File sync request
            # s3_key format: "deep-insight/fargate_sessions/{session_id}/input/file.csv"
            sync_request = {
                "action": "sync_data_from_s3",
                "bucket_name": S3_BUCKET_NAME,
                "s3_key_prefix": f"deep-insight/fargate_sessions/{s3_key.split('/')[2]}/input/",
                "local_path": "/app/data/"
            }

            # ‚úÖ 2. Request log
            logger.info(f"üì§ Sending file sync request:")
            logger.info(f"   URL: {alb_dns}/file-sync")
            logger.info(f"   Request: {sync_request}")

            # ‚úÖ Use per-request HTTP client (cookie isolation)
            http_client = self._get_http_client(self._current_request_id)
            response = http_client.post(
                f"http://{alb_dns}/file-sync",
                json=sync_request,
                timeout=self.FILE_SYNC_TIMEOUT
            )

            # ‚úÖ 3. Response log
            logger.info(f"üì• File sync response:")
            logger.info(f"   Status: {response.status_code}")
            logger.info(f"   Body: {response.text[:500]}")  # First 500 chars only

            if response.status_code != 200:
                logger.error(f"‚ùå File sync failed with status {response.status_code}")
                raise Exception(f"File sync failed: {response.text}")

            result = response.json()
            files_count = result.get('files_count', 0)
            downloaded_files = result.get('downloaded_files', [])

            # ‚úÖ 4. Result log
            logger.info(f"‚úÖ File sync completed:")
            logger.info(f"   Files synced: {files_count}")
            logger.info(f"   Downloaded: {downloaded_files}")

            # ‚úÖ 5. Wait start log
            logger.info(f"‚è≥ Waiting {self.FILE_SYNC_WAIT} seconds for file sync to complete...")
            time.sleep(self.FILE_SYNC_WAIT)

            # ‚úÖ 6. Wait complete log
            logger.info("‚úÖ File sync wait complete")

        except Exception as e:
            logger.error(f"‚ùå File sync failed: {e}")
            logger.error(f"   Exception type: {type(e).__name__}")
            logger.error(f"   Exception details: {str(e)[:1000]}")
            raise

    # ========================================================================
    # üßπ CLEANUP METHODS
    # ========================================================================

    def _cleanup_orphaned_containers(self):
        """Clean up only current request's container on session creation failure (protect other requests' containers)"""
        try:
            ecs_client = boto3.client('ecs', region_name=self._get_aws_region())

            # Check current request's Task ARN
            current_task_arn = None
            if self._current_request_id and self._current_request_id in self._sessions:
                session_info = self._sessions[self._current_request_id]
                fargate_session = session_info.get('fargate_session', {})
                current_task_arn = fargate_session.get('task_arn')

            if not current_task_arn:
                logger.warning(f"‚ö†Ô∏è No task ARN found for current request {self._current_request_id} - skipping cleanup")
                return

            # ‚úÖ Stop only current request's container (don't touch other requests' containers)
            try:
                logger.info(f"üßπ Cleaning up orphaned container for request {self._current_request_id}: {current_task_arn.split('/')[-1][:12]}...")
                ecs_client.stop_task(
                    cluster=ECS_CLUSTER_NAME,
                    task=current_task_arn,
                    reason=f'Session creation failed - cleanup (request: {self._current_request_id})'
                )
                logger.info(f"   ‚úÖ Stopped container: {current_task_arn.split('/')[-1][:12]}")
            except Exception as stop_error:
                logger.warning(f"   ‚ö†Ô∏è Failed to stop container {current_task_arn.split('/')[-1][:12]}: {stop_error}")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Orphaned container cleanup failed: {e}")

    def _deregister_from_alb(self, container_ip: str):
        """Remove container from ALB Target Group"""
        try:
            elbv2_client = boto3.client('elbv2', region_name=self._get_aws_region())
            elbv2_client.deregister_targets(
                TargetGroupArn=self._session_manager.alb_target_group_arn,
                Targets=[{
                    'Id': container_ip,
                    'Port': 8080
                }]
            )
            logger.info(f"üîó Deregistered target from ALB: {container_ip}:8080")
        except Exception as alb_error:
            # Continue session cleanup even if ALB deregistration fails
            logger.warning(f"‚ö†Ô∏è Failed to deregister ALB target {container_ip}: {alb_error}")

    def _auto_cleanup(self):
        """Automatically clean up all sessions on program exit"""
        try:
            if self._sessions:
                logger.info(f"üßπ Auto-cleanup: Closing {len(self._sessions)} Fargate sessions on exit...")
                # Clean up all sessions
                for request_id in list(self._sessions.keys()):
                    self.cleanup_session(request_id)

            # ‚úÖ Clear all HTTP clients
            if self._http_clients:
                logger.info(f"üßπ Auto-cleanup: Clearing {len(self._http_clients)} HTTP clients...")
                self._http_clients.clear()

            # ‚úÖ Clear all failure counters
            if self._session_creation_failures:
                logger.info(f"üßπ Auto-cleanup: Clearing {len(self._session_creation_failures)} failure counters...")
                self._session_creation_failures.clear()

            # ‚úÖ Clear all cleanup trackers
            if self._cleaned_up_requests:
                logger.info(f"üßπ Auto-cleanup: Clearing {len(self._cleaned_up_requests)} cleaned-up request trackers...")
                self._cleaned_up_requests.clear()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Auto-cleanup failed: {e}")


# ============================================================================
# GLOBAL INSTANCE (SINGLETON)
# ============================================================================

# Global instance (Singleton)
global_fargate_session = GlobalFargateSessionManager()


def get_global_session():
    """Return global session manager instance"""
    return global_fargate_session
