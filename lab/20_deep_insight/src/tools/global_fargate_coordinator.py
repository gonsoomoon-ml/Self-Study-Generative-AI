#!/usr/bin/env python3
"""
Global Fargate Session Manager for Tools
ë„êµ¬ë“¤ì´ ê³µìœ í•˜ëŠ” ê¸€ë¡œë²Œ ì„¸ì…˜ ë§¤ë‹ˆì €

This module provides a singleton session manager for coordinating Fargate container
sessions across multiple tools and concurrent requests. It handles:
- Multi-request session isolation with separate HTTP clients and cookies
- Container lifecycle management (creation, health checks, cleanup)
- ALB sticky session management with subprocess-based cookie acquisition
- S3 data synchronization for CSV files
- Automatic cleanup on program exit
"""

# ============================================================================
# IMPORTS
# ============================================================================

import logging
import os
import time
import json
import subprocess
import atexit
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables (don't override Runtime env vars)
load_dotenv(override=False)

# ECS and ALB configuration from environment
# These must be provided via environment variables (no hardcoded defaults)
ECS_CLUSTER_NAME = os.getenv("ECS_CLUSTER_NAME")
ALB_TARGET_GROUP_ARN = os.getenv("ALB_TARGET_GROUP_ARN")
S3_BUCKET_NAME = os.getenv("S3_BUCKET_NAME")

# Third-party imports
import boto3
import requests
from botocore.exceptions import ClientError

# Local imports
from src.tools.fargate_container_controller import SessionBasedFargateManager

# ============================================================================
# LOGGER SETUP
# ============================================================================

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


# ============================================================================
# GLOBAL FARGATE SESSION MANAGER (SINGLETON)
# ============================================================================

class GlobalFargateSessionManager:
    """
    Singleton session manager for coordinating Fargate container sessions.

    Features:
    - Per-request session isolation (cookies, HTTP clients, containers)
    - Exponential backoff retry for session creation
    - ALB health checks and sticky session management
    - S3 data synchronization
    - Automatic cleanup on exit
    """

    # ========================================================================
    # CLASS VARIABLES (SINGLETON STATE)
    # ========================================================================

    _instance = None
    _session_manager = None
    _sessions = {}  # {request_id: session_info} - ìš”ì²­ë³„ ì„¸ì…˜ ê´€ë¦¬
    _http_clients = {}  # {request_id: http_session} - ìš”ì²­ë³„ HTTP í´ë¼ì´ì–¸íŠ¸ (ì¿ í‚¤ ê²©ë¦¬)
    _used_container_ips = {}  # {container_ip: request_id} - IP ê¸°ë°˜ ì»¨í…Œì´ë„ˆ ì†Œìœ ê¶Œ ì¶”ì 
    _current_request_id = None  # í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ì˜ ìš”ì²­ ID
    _retry_count = 0
    _max_retries = 2
    _session_creation_failures = {}  # {request_id: failure_count} - ì„¸ì…˜ ìƒì„± ì‹¤íŒ¨ íšŸìˆ˜ ì¶”ì 
    _max_session_failures = 5  # ìµœëŒ€ ì„¸ì…˜ ìƒì„± ì‹¤íŒ¨ í—ˆìš© íšŸìˆ˜ (ECS Task Limit ëŒ€ì‘)
    _cleaned_up_requests = set()  # ì´ë¯¸ cleanupëœ ìš”ì²­ ID ì¶”ì  (ì¬ìƒì„± ë°©ì§€)

    # ========================================================================
    # ğŸ“¦ INITIALIZATION (SINGLETON PATTERN)
    # ========================================================================

    def __new__(cls):
        """Singleton pattern implementation"""
        if cls._instance is None:
            cls._instance = super(GlobalFargateSessionManager, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        """Initialize the session manager (only once due to singleton)"""
        if self._session_manager is None:
            logger.info("ğŸš€ Initializing Global Fargate Session Manager")
            self._session_manager = SessionBasedFargateManager()
            atexit.register(self._auto_cleanup)

    # ========================================================================
    # ğŸŒ PUBLIC API METHODS
    # ========================================================================

    def set_request_context(self, request_id: str):
        """í˜„ì¬ ìš”ì²­ ì»¨í…ìŠ¤íŠ¸ ì„¤ì •"""
        self._current_request_id = request_id
        logger.info(f"ğŸ“‹ Request context set: {request_id}")

    def ensure_session(self):
        """
        ì„¸ì…˜ì´ ì—†ê±°ë‚˜ ë¹„í™œì„±í™”ëœ ê²½ìš° ìƒˆ ì„¸ì…˜ ìƒì„± (ìš”ì²­ë³„ ì„¸ì…˜ ê´€ë¦¬, Exponential Backoff ì ìš©)

        Returns:
            bool: True if session exists or was created successfully, False otherwise
        """
        try:
            if not self._current_request_id:
                raise Exception("Request context not set. Call set_request_context() first.")

            # âœ… ì´ë¯¸ cleanupëœ ìš”ì²­ì€ ìƒˆ ì„¸ì…˜ ìƒì„± ê¸ˆì§€ (ì¤‘ë³µ ì»¨í…Œì´ë„ˆ ë°©ì§€)
            if self._current_request_id in self._cleaned_up_requests:
                error_msg = f"âŒ FATAL: Request {self._current_request_id} already cleaned up - cannot create new session. This prevents duplicate container creation after workflow completion."
                logger.error(error_msg)
                raise Exception(error_msg)

            # í˜„ì¬ ìš”ì²­ì˜ ì„¸ì…˜ í™•ì¸
            if self._current_request_id in self._sessions:
                return self._reuse_existing_session()

            # ìƒˆ ì„¸ì…˜ ìƒì„± (Exponential Backoff ì ìš©)
            return self._create_new_session()

        except Exception as e:
            logger.error(f"âŒ Failed to ensure session: {e}")

            # âœ… ì¹˜ëª…ì  ì—ëŸ¬ëŠ” ì¬ë°œìƒ (ì›Œí¬í”Œë¡œìš° ì¤‘ë‹¨)
            if "FATAL" in str(e):
                raise

            return False

    def ensure_session_with_data(self, csv_file_path: str):
        """
        CSV íŒŒì¼ê³¼ í•¨ê»˜ ì„¸ì…˜ ìƒì„± (ì„¸ì…˜ í™•ì¸ â†’ S3 ì—…ë¡œë“œ â†’ ì»¨í…Œì´ë„ˆ ë™ê¸°í™”)

        Args:
            csv_file_path: Path to CSV file to upload

        Returns:
            bool: True if successful, False otherwise
        """
        try:
            logger.info(f"ğŸš€ Creating session with data: {csv_file_path}")

            # âœ… 1. ë¨¼ì € ì„¸ì…˜ ìƒì„± (Timestamp ìƒì„±)
            if not self.ensure_session():
                raise Exception("Failed to create Fargate session")

            # âœ… 2. ìƒì„±ëœ ì„¸ì…˜ IDë¥¼ ì‚¬ìš©í•˜ì—¬ S3 ì—…ë¡œë“œ
            session_id = self._sessions[self._current_request_id]['session_id']
            s3_key = self._upload_csv_to_s3_with_session_id(csv_file_path, session_id)
            logger.info(f"ğŸ“¤ CSV uploaded to S3: {s3_key}")

            # 3. ì»¨í…Œì´ë„ˆì— S3 â†’ ë¡œì»¬ ë™ê¸°í™”
            self._sync_csv_from_s3_to_container(s3_key)
            logger.info("âœ… CSV file synced to container")

            return True

        except Exception as e:
            logger.error(f"âŒ Failed to create session with data: {e}")
            return False

    def execute_code(self, code: str, description: str = ""):
        """
        ì½”ë“œ ì‹¤í–‰ with ìë™ ì„¸ì…˜ ê´€ë¦¬ ë° ì—°ê²° ì¬ì‹œë„

        Args:
            code: Python code to execute
            description: Description of the code execution

        Returns:
            dict: Execution result or error
        """
        max_retries = 3
        retry_delay = 2  # seconds

        for attempt in range(1, max_retries + 1):
            try:
                # ì„¸ì…˜ í™•ì¸ ë° ìƒì„±
                if not self.ensure_session():
                    return {"error": "Failed to create or maintain session"}

                # ì½”ë“œ ì‹¤í–‰
                result = self._session_manager.execute_code(code, description)

                # ì„±ê³µí•˜ë©´ ë°”ë¡œ ë°˜í™˜
                return result

            except Exception as e:
                error_msg = str(e)

                # ì—°ê²° ê´€ë ¨ ì—ëŸ¬ì¸ì§€ í™•ì¸
                is_connection_error = any(keyword in error_msg.upper() for keyword in [
                    "CONNECTION FAILED",
                    "NOT RESPONDING",
                    "TIMEOUT",
                    "CONNECTIONERROR",
                    "HTTPERROR"
                ])

                if is_connection_error:
                    # ì—°ê²° ì—ëŸ¬ - ì¬ì‹œë„
                    logger.warning(f"âš ï¸ Connection error (attempt {attempt}/{max_retries}): {error_msg}")

                    if attempt < max_retries:
                        logger.info(f"ğŸ”„ Retrying in {retry_delay} seconds...")
                        time.sleep(retry_delay)
                    else:
                        logger.error(f"âŒ Connection failed after {max_retries} attempts. Giving up.")
                        return {
                            "error": f"Connection failed after {max_retries} attempts: {error_msg}"
                        }
                else:
                    # ì½”ë“œ ì‹¤í–‰ ì—ëŸ¬ ë“± - ì¬ì‹œë„ ì•ˆ í•¨
                    logger.error(f"âŒ Code execution failed: {e}")
                    # âš ï¸ ì„¸ì…˜ì„ Noneìœ¼ë¡œ ë¦¬ì…‹í•˜ì§€ ì•ŠìŒ!
                    # ì»¨í…Œì´ë„ˆ í†µì‹  ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ë‹¤ìŒ Agentê°€ ì¬ì‹œë„í•  ìˆ˜ ìˆë„ë¡ ì„¸ì…˜ ìœ ì§€
                    # ì—¬ëŸ¬ Agentê°€ ê°™ì€ ì»¨í…Œì´ë„ˆë¥¼ ê³µìœ í•´ì•¼ í•˜ë¯€ë¡œ ì„¸ì…˜ ì´ˆê¸°í™” ê¸ˆì§€
                    return {"error": str(e)}

    def cleanup_session(self, request_id: str = None):
        """
        íŠ¹ì • ìš”ì²­ì˜ ì„¸ì…˜ ì •ë¦¬

        Args:
            request_id: Request ID to cleanup (defaults to current request)
        """
        try:
            # request_idê°€ ì—†ìœ¼ë©´ í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©
            cleanup_request_id = request_id or self._current_request_id

            if not cleanup_request_id:
                logger.warning("âš ï¸ No request ID for cleanup")
                return

            if cleanup_request_id in self._sessions:
                session_info = self._sessions[cleanup_request_id]
                logger.info(f"ğŸ§¹ Cleaning up session for request {cleanup_request_id}: {session_info['session_id']}")

                container_ip = session_info.get('container_ip')

                # âœ… FIX: complete_session()ì„ ë¨¼ì € í˜¸ì¶œ (ALB ì œê±° ì „ì—)
                # 1. ë¨¼ì € ì»¨í…Œì´ë„ˆê°€ S3ì— ì—…ë¡œë“œí•˜ë„ë¡ í—ˆìš©
                logger.info(f"ğŸ Completing session (S3 upload)...")
                self._session_manager.current_session = session_info['fargate_session']
                self._session_manager.complete_session()

                # 2. ê·¸ ë‹¤ìŒ ì»¨í…Œì´ë„ˆ IP í•´ì œ ë° ALB ì œê±° (ì´ì œ ì•ˆì „í•¨)
                if container_ip and container_ip in self._used_container_ips:
                    del self._used_container_ips[container_ip]
                    logger.info(f"ğŸ§¹ Released container IP: {container_ip}")
                    logger.info(f"   Remaining IPs: {list(self._used_container_ips.keys())}")

                    # âœ… ALB Target Groupì—ì„œ ì»¨í…Œì´ë„ˆ ì œê±° (zombie target ë°©ì§€)
                    # complete_session() ì´í›„ì— ì‹¤í–‰í•˜ì—¬ HTTP 502 ì—ëŸ¬ ë°©ì§€
                    self._deregister_from_alb(container_ip)

                # ì„¸ì…˜ ë”•ì…”ë„ˆë¦¬ì—ì„œ ì œê±°
                del self._sessions[cleanup_request_id]
                logger.info(f"âœ… Session cleanup completed. Remaining sessions: {len(self._sessions)}")
            else:
                logger.warning(f"âš ï¸ No session found for request {cleanup_request_id}")

            # âœ… HTTP í´ë¼ì´ì–¸íŠ¸ë„ ì •ë¦¬ (ì¿ í‚¤ ì œê±°)
            if cleanup_request_id in self._http_clients:
                del self._http_clients[cleanup_request_id]
                logger.info(f"ğŸª Removed HTTP client for request {cleanup_request_id}")

            # âœ… ì‹¤íŒ¨ ì¹´ìš´í„°ë„ ì •ë¦¬
            if cleanup_request_id in self._session_creation_failures:
                del self._session_creation_failures[cleanup_request_id]
                logger.info(f"ğŸ§¹ Cleared failure counter for request {cleanup_request_id}")

            # âœ… cleanupëœ ìš”ì²­ IDë¥¼ ì¶”ì  (ì¬ìƒì„± ë°©ì§€)
            self._cleaned_up_requests.add(cleanup_request_id)
            logger.info(f"ğŸ”’ Request {cleanup_request_id} marked as cleaned up - new session creation blocked")

        except Exception as e:
            logger.error(f"âŒ Session cleanup failed: {e}")

    # ========================================================================
    # ğŸ”§ SESSION MANAGEMENT (PRIVATE HELPERS)
    # ========================================================================

    def _reuse_existing_session(self):
        """ê¸°ì¡´ ì„¸ì…˜ ì¬ì‚¬ìš© (í—¬ìŠ¤ ì²´í¬ í¬í•¨)"""
        session_info = self._sessions[self._current_request_id]
        container_ip = session_info.get('container_ip', 'unknown')

        logger.info(f"â™»ï¸ Reusing existing session for request {self._current_request_id}: {session_info['session_id']}")

        # ğŸ” Container Health Check (ALB Target Health)
        if container_ip != 'unknown':
            target_health = self._check_alb_target_health(container_ip)
            logger.info(f"   ğŸ¥ Container ALB Health: {target_health}")

            if target_health not in ['healthy', 'initial']:
                logger.warning(f"âš ï¸ WARNING: Reusing session with container in '{target_health}' state!")
                logger.warning(f"   This may cause connection failures!")
                logger.warning(f"   Container IP: {container_ip}")
                logger.warning(f"   Session ID: {session_info['session_id']}")
                logger.warning(f"   Consider implementing automatic cleanup for stopped containers")

        # SessionBasedFargateManagerì˜ current_session ì—…ë°ì´íŠ¸
        self._session_manager.current_session = session_info['fargate_session']

        # âœ… HTTP Sessionë„ ì¬ì£¼ì… (ì„¸ì…˜ ì¬ì‚¬ìš© ì‹œì—ë„ í•„ìš”)
        http_client = self._get_http_client(self._current_request_id)
        self._session_manager.set_http_session(http_client)

        return True

    def _create_new_session(self):
        """ìƒˆ ì„¸ì…˜ ìƒì„± (Exponential Backoff ì ìš©)"""
        for attempt in range(1, self._max_session_failures + 1):
            try:
                # ğŸ” ë™ì‹œ ì‹¤í–‰ ê°ì§€ ë¡œê·¸
                active_sessions = [req_id for req_id in self._sessions.keys() if req_id not in self._cleaned_up_requests]
                logger.info(f"ğŸ“¦ Creating new Fargate session for request {self._current_request_id} (attempt {attempt}/{self._max_session_failures})...")
                logger.info(f"   Current active sessions: {len(active_sessions)}")
                if active_sessions:
                    logger.info(f"   Active request IDs: {active_sessions}")
                    logger.info(f"   Active container IPs: {[self._sessions[req_id]['container_ip'] for req_id in active_sessions if req_id in self._sessions]}")

                timestamp_id = datetime.now().strftime("%Y-%m-%d-%H-%M-%S")

                fargate_session_info = self._session_manager.create_session(
                    session_id=timestamp_id,
                    max_executions=300
                )

                # âœ… HTTP Session ì£¼ì… (ìš”ì²­ë³„ ì¿ í‚¤ ê²©ë¦¬)
                http_client = self._get_http_client(self._current_request_id)
                self._session_manager.set_http_session(http_client)
                logger.info(f"ğŸ”— HTTP session injected for request {self._current_request_id}")

                # âœ… ì»¨í…Œì´ë„ˆ IP ë“±ë¡ (AWS VPCê°€ ìœ ë‹ˆí¬í•œ IP ë³´ì¥)
                expected_private_ip = self._session_manager.current_session['private_ip']
                self._used_container_ips[expected_private_ip] = self._current_request_id
                logger.info(f"ğŸ“ Registered container IP: {expected_private_ip}")
                logger.info(f"   Request ID: {self._current_request_id}")
                logger.info(f"   All registered IPs: {list(self._used_container_ips.keys())}")

                # âš ï¸ FIX: ì„¸ì…˜ ì €ì¥ì„ Health Check ì™„ë£Œ í›„ë¡œ ì´ë™ (Race Condition ë°©ì§€)
                # ì´ ì‹œì ì—ëŠ” self._sessionsì— ì €ì¥í•˜ì§€ ì•ŠìŒ!
                # â†’ ensure_session() ë™ì‹œ í˜¸ì¶œ ì‹œ unhealthy ì„¸ì…˜ ì¬ì‚¬ìš© ë°©ì§€

                # Wait for ALB health check and acquire cookie
                if not self._wait_for_container_ready(expected_private_ip, fargate_session_info['session_id']):
                    # ì‹¤íŒ¨ ì‹œ IP ë“±ë¡ í•´ì œ
                    if expected_private_ip in self._used_container_ips:
                        del self._used_container_ips[expected_private_ip]
                    return False

                # âœ… FIX: Health Check + Cookie íšë“ ì™„ë£Œ í›„ ì„¸ì…˜ ì €ì¥ (Race Condition ë°©ì§€)
                # ì´ì œ ì•ˆì „í•˜ê²Œ ì„¸ì…˜ì„ self._sessionsì— ì €ì¥
                # â†’ ë‹¤ë¥¸ ìŠ¤ë ˆë“œê°€ ensure_session() í˜¸ì¶œ ì‹œ healthy ì„¸ì…˜ë§Œ ì¬ì‚¬ìš©
                self._sessions[self._current_request_id] = {
                    'session_id': fargate_session_info['session_id'],
                    'request_id': self._current_request_id,
                    'container_ip': expected_private_ip,
                    'fargate_session': self._session_manager.current_session,
                    'created_at': datetime.now()
                }
                logger.info(f"âœ… Session created and saved for request {self._current_request_id}: {fargate_session_info['session_id']}")
                logger.info(f"   Total active sessions: {len(self._sessions)}")

                # âœ… ì„¸ì…˜ ìƒì„± ì„±ê³µ - ì‹¤íŒ¨ ì¹´ìš´í„° ë¦¬ì…‹
                if self._current_request_id in self._session_creation_failures:
                    del self._session_creation_failures[self._current_request_id]

                self._retry_count = 0
                return True

            except ClientError as create_error:
                error_code = create_error.response['Error']['Code']
                error_message = create_error.response['Error']['Message']
                logger.error(f"âŒ Session creation failed (attempt {attempt}/{self._max_session_failures}): [{error_code}] {error_message}")

                # ì„¸ì…˜ ìƒì„± ìì²´ê°€ ì‹¤íŒ¨í•œ ê²½ìš°ë§Œ cleanup
                if self._current_request_id in self._sessions:
                    del self._sessions[self._current_request_id]
                self._cleanup_orphaned_containers()

                # âŒ Configuration errors - FAIL FAST (don't retry)
                NON_RETRYABLE_ERRORS = [
                    'ValidationException',  # Invalid parameters (e.g., wrong VPC, subnet, etc.)
                    'InvalidParameterException',  # Invalid parameters
                    'AccessDeniedException',  # IAM permission issues
                    'ResourceNotFoundException',  # Resource not found
                    'UnauthorizedException',  # Auth issues
                ]

                if error_code in NON_RETRYABLE_ERRORS:
                    logger.error(f"âŒ FATAL: Non-retryable configuration error detected: {error_code}")
                    logger.error(f"   Error: {error_message}")
                    logger.error(f"   Fix the configuration and try again. Not retrying.")
                    failure_count = self._session_creation_failures.get(self._current_request_id, 0)
                    self._session_creation_failures[self._current_request_id] = failure_count + 1
                    raise

                # âœ… Transient errors - retry with exponential backoff
                # (e.g., ThrottlingException, ServiceUnavailable, etc.)
                if attempt < self._max_session_failures:
                    wait_time = 3 ** attempt  # 3, 9, 27, 81ì´ˆ
                    logger.warning(f"â³ Transient error - waiting {wait_time}s before retry (exponential backoff: 3^{attempt})...")
                    time.sleep(wait_time)
                else:
                    # ë§ˆì§€ë§‰ ì‹œë„ ì‹¤íŒ¨ - ì‹¤íŒ¨ ì¹´ìš´í„° ì¦ê°€ í›„ ì—ëŸ¬ ë°œìƒ
                    failure_count = self._session_creation_failures.get(self._current_request_id, 0)
                    self._session_creation_failures[self._current_request_id] = failure_count + 1
                    logger.error(f"âŒ FATAL: Session creation failed {self._max_session_failures} times for request {self._current_request_id}")
                    logger.error(f"   Total backoff time: {3 + 9 + 27 + 81} seconds")
                    raise

            except Exception as create_error:
                # Handle non-AWS exceptions (e.g., network errors, Python exceptions)
                logger.error(f"âŒ Session creation failed (attempt {attempt}/{self._max_session_failures}): {create_error}")

                # ì„¸ì…˜ ìƒì„± ìì²´ê°€ ì‹¤íŒ¨í•œ ê²½ìš°ë§Œ cleanup
                if self._current_request_id in self._sessions:
                    del self._sessions[self._current_request_id]
                self._cleanup_orphaned_containers()

                # Retry non-AWS exceptions
                if attempt < self._max_session_failures:
                    wait_time = 3 ** attempt  # 3, 9, 27, 81ì´ˆ
                    logger.warning(f"â³ Waiting {wait_time}s before retry (exponential backoff: 3^{attempt})...")
                    time.sleep(wait_time)
                else:
                    # ë§ˆì§€ë§‰ ì‹œë„ ì‹¤íŒ¨ - ì‹¤íŒ¨ ì¹´ìš´í„° ì¦ê°€ í›„ ì—ëŸ¬ ë°œìƒ
                    failure_count = self._session_creation_failures.get(self._current_request_id, 0)
                    self._session_creation_failures[self._current_request_id] = failure_count + 1
                    logger.error(f"âŒ FATAL: Session creation failed {self._max_session_failures} times for request {self._current_request_id}")
                    logger.error(f"   Total backoff time: {3 + 9 + 27 + 81} seconds")
                    raise

    def _wait_for_container_ready(self, expected_ip: str, session_id: str) -> bool:
        """ì»¨í…Œì´ë„ˆ ì¤€ë¹„ ëŒ€ê¸° (ALB Health Check + Cookie íšë“)"""
        # ğŸ› DEBUG: Checkpoint before health check
        logger.info(f"ğŸ” DEBUG: About to start ALB health check wait for {expected_ip}")

        # ğŸ†• ALBê°€ Health Checkë¥¼ ì‹œì‘í•  ì‹œê°„ í™•ë³´ (60ì´ˆ ëŒ€ê¸°, keep-alive ë¡œê·¸)
        logger.info(f"â³ Waiting 60 seconds for ALB to begin health checks...")
        logger.info(f"   This prevents 'ALB never sent health checks' issue")

        # Keep-alive: 60ì´ˆë¥¼ 6ë²ˆì˜ 10ì´ˆë¡œ ë‚˜ëˆ„ì–´ ì¤‘ê°„ì— ë¡œê·¸ ì¶œë ¥
        for wait_i in range(6):
            time.sleep(10)
            logger.info(f"   â±ï¸  Waiting for ALB... ({(wait_i+1)*10}/60s)")

        # â° ALB Health Check ëŒ€ê¸° (Containerê°€ healthy ìƒíƒœê°€ ë  ë•Œê¹Œì§€)
        logger.info(f"â° Waiting for container {expected_ip} to be healthy in ALB...")
        alb_healthy = False
        for wait_attempt in range(1, 31):  # ìµœëŒ€ 150ì´ˆ (30 * 5s)
            target_health = self._check_alb_target_health(expected_ip)
            logger.info(f"   Attempt {wait_attempt}/30: ALB health = {target_health}")

            if target_health == 'healthy':
                logger.info(f"âœ… Container is healthy in ALB after {wait_attempt * 5}s")
                alb_healthy = True
                break
            elif target_health in ['unhealthy', 'draining']:
                logger.warning(f"âš ï¸ Container is {target_health} - continuing to wait...")
            elif target_health == 'not_registered':
                logger.info(f"   Container not yet registered to ALB - waiting...")

            if wait_attempt < 30:
                time.sleep(5)

        if not alb_healthy:
            logger.warning(f"âš ï¸ Container not healthy after 150s, but will try cookie acquisition anyway")

        # ğŸª IP ê¸°ë°˜ ì¿ í‚¤ íšë“ (ì„¸ì…˜ ID ê²€ì¦ í¬í•¨)
        cookie_acquired = self._acquire_cookie_for_ip(expected_ip, session_id)

        if not cookie_acquired:
            logger.warning(f"âš ï¸ Failed to acquire Sticky Session cookie")
            logger.warning(f"   Releasing IP registration: {expected_ip}")
            logger.warning(f"   Cookie acquisition failed - session NOT saved")
            return False

        return True

    def _get_http_client(self, request_id: str):
        """ìš”ì²­ë³„ HTTP í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜ (ì¿ í‚¤ ê²©ë¦¬)"""
        if request_id not in self._http_clients:
            self._http_clients[request_id] = requests.Session()
            logger.info(f"ğŸª Created new HTTP client for request {request_id}")
        return self._http_clients[request_id]

    def _check_alb_target_health(self, target_ip: str) -> str:
        """
        Check if the specified IP is registered and healthy in the ALB target group

        Returns:
            'healthy', 'unhealthy', 'initial', 'draining', 'unused', 'not_registered', 'unknown'
        """
        try:
            if not ALB_TARGET_GROUP_ARN:
                raise ValueError("ALB_TARGET_GROUP_ARN environment variable is required")

            elbv2_client = boto3.client('elbv2', region_name='us-east-1')
            response = elbv2_client.describe_target_health(TargetGroupArn=ALB_TARGET_GROUP_ARN)

            for target_health in response.get('TargetHealthDescriptions', []):
                if target_health['Target']['Id'] == target_ip:
                    state = target_health['TargetHealth']['State']
                    return state

            return 'not_registered'
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to check ALB target health: {e}")
            return 'unknown'

    # ========================================================================
    # ğŸª COOKIE ACQUISITION (SUBPROCESS-BASED)
    # ========================================================================

    def _acquire_cookie_for_ip(self, expected_ip: str, session_id: str) -> bool:
        """
        subprocessë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • IPì˜ ì»¨í…Œì´ë„ˆë¡œë¶€í„° Sticky Session ì¿ í‚¤ íšë“

        subprocess ì‚¬ìš© ì´ìœ :
        - ì™„ì „í•œ í”„ë¡œì„¸ìŠ¤ ê²©ë¦¬ â†’ ë…ë¦½ì ì¸ TCP Connection Pool
        - ALB Round Robinì´ ì •ìƒ ì‘ë™ (Connection: close/Pool Control ë¶ˆí•„ìš”)
        - ë©€í‹° Job í™˜ê²½ì—ì„œ ê° Jobì´ ë…ë¦½ì ìœ¼ë¡œ ì¿ í‚¤ íšë“ ê°€ëŠ¥

        ì‘ë™ ì›ë¦¬:
        1. ë…ë¦½ëœ Python í”„ë¡œì„¸ìŠ¤ë¡œ ì¸ë¼ì¸ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        2. subprocess ë‚´ë¶€ì—ì„œ 40ë²ˆ ì‹œë„ (ê°ê° ìƒˆë¡œìš´ TCP ì—°ê²°)
        3. ALB Round Robinìœ¼ë¡œ ëª©í‘œ ì»¨í…Œì´ë„ˆ ë„ë‹¬ ì‹œ ì¿ í‚¤ íšë“
        4. ì„¸ì…˜ ID ê²€ì¦ì„ í†µí•´ ì˜¬ë°”ë¥¸ ì»¨í…Œì´ë„ˆì¸ì§€ í™•ì¸ (ë©€í‹° Job ì§€ì›)
        5. JSON í˜•íƒœë¡œ ê²°ê³¼ ë°˜í™˜ (stdout)
        6. Parent processì—ì„œ ì¿ í‚¤ë¥¼ HTTP Clientì— ì„¤ì •
        """
        # âœ… CloudWatch ê³µì¸ IP ë¡œê¹… (AgentCore Runtime â†’ ALB ì²« ì—°ê²°)
        # ALB Security Groupì´ ì‹¤ì œë¡œ ê²€ì¦í•˜ëŠ” ê³µì¸ IP (NAT Gateway IP) ê¸°ë¡
        # NOTE: Disabled to keep all traffic 100% private (no internet access needed)
        # try:
        #     # AWS checkip ì„œë¹„ìŠ¤ë¥¼ í†µí•´ ê³µì¸ IP í™•ì¸
        #     response = requests.get("https://checkip.amazonaws.com", timeout=3)
        #     public_ip = response.text.strip()
        #     logger.info(f"ğŸŒğŸŒğŸŒ PUBLIC IP DETECTED ğŸŒğŸŒğŸŒ First ALB connection from public IP: {public_ip} to ALB: {self._session_manager.alb_dns}")
        # except Exception as ip_err:
        #     logger.warning(f"âš ï¸ Failed to detect public IP: {ip_err}")
        #     public_ip = "unknown"

        logger.info(f"ğŸª Acquiring cookie for container: {expected_ip}")
        logger.info(f"   Session ID: {session_id}")
        other_ips = [ip for ip in self._used_container_ips.keys() if ip != expected_ip]
        if other_ips:
            logger.info(f"   Other active containers: {other_ips}")
        else:
            logger.info(f"   No other active containers")

        try:
            # Get path to cookie acquisition subprocess script
            current_file = Path(__file__)
            script_path = current_file.parent / "cookie_acquisition_subprocess.py"

            if not script_path.exists():
                raise FileNotFoundError(f"Cookie acquisition script not found: {script_path}")

            # subprocessë¡œ ë³„ë„ íŒŒì¼ ì‹¤í–‰
            logger.info(f"ğŸ”§ Launching subprocess for cookie acquisition...")
            logger.info(f"   Script: {script_path}")

            result = subprocess.run(
                ["python3", str(script_path), self._session_manager.alb_dns, expected_ip, session_id],
                capture_output=True,
                text=True,
                timeout=240  # 4ë¶„ (40 attempts * 5s = 200s + buffer)
            )

            # âœ… Subprocess stderr ë¡œê¹… (ì§„í–‰ ìƒí™© ë° ë””ë²„ê¹… ì •ë³´)
            if result.stderr:
                for line in result.stderr.strip().split('\n'):
                    if line.strip():
                        logger.info(f"   {line}")

            # stdout íŒŒì‹±
            output = result.stdout.strip()
            if not output:
                logger.error(f"âŒ Subprocess produced no output")
                if result.stderr:
                    logger.error(f"   Full stderr: {result.stderr}")
                return False

            data = json.loads(output)

            if data.get("success"):
                cookie_value = data.get("cookie")
                attempt = data.get("attempt")
                actual_ip = data.get("ip")

                logger.info(f"âœ… Cookie acquired! (attempt {attempt})")
                logger.info(f"   My container IP: {actual_ip}")
                logger.info(f"   Cookie value: {cookie_value[:20]}...")

                # HTTP Clientì— ì¿ í‚¤ ì„¤ì •
                http_client = requests.Session()
                http_client.cookies.set('AWSALB', cookie_value)

                # ì €ì¥
                self._http_clients[self._current_request_id] = http_client
                return True
            else:
                error_msg = data.get("error", "Unknown error")
                logger.error(f"âŒ Cookie acquisition failed: {error_msg}")
                logger.error(f"   Expected IP: {expected_ip}")
                logger.error(f"   Registered IPs: {list(self._used_container_ips.keys())}")
                return False

        except subprocess.TimeoutExpired as e:
            logger.error(f"âŒ Cookie acquisition timeout (4 minutes)")
            logger.error(f"   Expected IP: {expected_ip}")
            if e.stderr:
                # âœ… Timeout stderrë„ ë¡œê¹… (bytes â†’ string ì•ˆì „ ë³€í™˜)
                stderr_text = str(e.stderr, 'utf-8') if isinstance(e.stderr, bytes) else e.stderr
                for line in stderr_text.strip().split('\n'):
                    if line.strip():
                        logger.error(f"   {line}")
            return False
        except json.JSONDecodeError as e:
            logger.error(f"âŒ Failed to parse subprocess output: {e}")
            logger.error(f"   Output: {result.stdout}")
            if result.stderr:
                logger.error(f"   Full stderr: {result.stderr}")
            return False
        except Exception as e:
            logger.error(f"âŒ Cookie acquisition subprocess failed: {e}")
            logger.error(f"   Expected IP: {expected_ip}")
            return False

    # ========================================================================
    # ğŸ“¤ DATA SYNC METHODS (S3 UPLOAD/DOWNLOAD)
    # ========================================================================

    def _upload_csv_to_s3_with_session_id(self, csv_file_path: str, session_id: str) -> str:
        """ì„¸ì…˜ IDë¥¼ ë°›ì•„ì„œ S3ì— ì—…ë¡œë“œ (Timestamp ë¶ˆì¼ì¹˜ ë°©ì§€)"""
        try:
            # âœ… ì„¸ì…˜ IDë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ìƒˆ timestamp ìƒì„± ì•ˆí•¨)
            original_filename = os.path.basename(csv_file_path)
            s3_key = f"manus/fargate_sessions/{session_id}/input/{original_filename}"

            # S3 ì—…ë¡œë“œ
            s3_client = boto3.client('s3', region_name='us-east-1')
            s3_client.upload_file(
                csv_file_path,
                S3_BUCKET_NAME,
                s3_key,
                ExtraArgs={'ContentType': 'text/csv'}
            )

            logger.info(f"ğŸ“¤ Uploaded {csv_file_path} â†’ s3://{S3_BUCKET_NAME}/{s3_key}")
            return s3_key

        except Exception as e:
            logger.error(f"âŒ S3 upload failed: {e}")
            raise

    def _sync_csv_from_s3_to_container(self, s3_key: str):
        """S3ì—ì„œ ì»¨í…Œì´ë„ˆë¡œ CSV íŒŒì¼ ë™ê¸°í™” (Enhanced Logging)"""
        try:
            # ALB DNS (ì„¸ì…˜ ë§¤ë‹ˆì €ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
            alb_dns = self._session_manager.alb_dns
            filename = s3_key.split('/')[-1]

            # âœ… 1. ì‹œì‘ ë¡œê·¸
            logger.info(f"ğŸ”„ Starting file sync...")
            logger.info(f"   S3 Key: {s3_key}")
            logger.info(f"   Filename: {filename}")
            logger.info(f"   Target: /app/data/{filename}")

            # íŒŒì¼ ë™ê¸°í™” ìš”ì²­
            # s3_key í˜•íƒœ: "manus/fargate_sessions/{session_id}/input/file.csv"
            sync_request = {
                "action": "sync_data_from_s3",
                "bucket_name": S3_BUCKET_NAME,
                "s3_key_prefix": f"manus/fargate_sessions/{s3_key.split('/')[2]}/input/",
                "local_path": "/app/data/"
            }

            # âœ… 2. ìš”ì²­ ë¡œê·¸
            logger.info(f"ğŸ“¤ Sending file sync request:")
            logger.info(f"   URL: {alb_dns}/file-sync")
            logger.info(f"   Request: {sync_request}")

            # âœ… ìš”ì²­ë³„ HTTP í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© (ì¿ í‚¤ ê²©ë¦¬)
            http_client = self._get_http_client(self._current_request_id)
            response = http_client.post(
                f"http://{alb_dns}/file-sync",
                json=sync_request,
                timeout=30
            )

            # âœ… 3. ì‘ë‹µ ë¡œê·¸
            logger.info(f"ğŸ“¥ File sync response:")
            logger.info(f"   Status: {response.status_code}")
            logger.info(f"   Body: {response.text[:500]}")  # ì²˜ìŒ 500ìë§Œ

            if response.status_code != 200:
                logger.error(f"âŒ File sync failed with status {response.status_code}")
                raise Exception(f"File sync failed: {response.text}")

            result = response.json()
            files_count = result.get('files_count', 0)
            downloaded_files = result.get('downloaded_files', [])

            # âœ… 4. ê²°ê³¼ ë¡œê·¸
            logger.info(f"âœ… File sync completed:")
            logger.info(f"   Files synced: {files_count}")
            logger.info(f"   Downloaded: {downloaded_files}")

            # âœ… 5. ëŒ€ê¸° ì‹œì‘ ë¡œê·¸
            logger.info("â³ Waiting 10 seconds for file sync to complete...")
            time.sleep(10)  # 10ì´ˆ ëŒ€ê¸° (íŒŒì¼ ë™ê¸°í™” ì™„ë£Œ ì‹œê°„)

            # âœ… 6. ëŒ€ê¸° ì™„ë£Œ ë¡œê·¸
            logger.info("âœ… File sync wait complete")

        except Exception as e:
            logger.error(f"âŒ File sync failed: {e}")
            logger.error(f"   Exception type: {type(e).__name__}")
            logger.error(f"   Exception details: {str(e)[:1000]}")
            raise

    # ========================================================================
    # ğŸ§¹ CLEANUP METHODS
    # ========================================================================

    def _cleanup_orphaned_containers(self):
        """ì„¸ì…˜ ìƒì„± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ìš”ì²­ì˜ ì»¨í…Œì´ë„ˆë§Œ ì •ë¦¬ (ë‹¤ë¥¸ ìš”ì²­ì˜ ì»¨í…Œì´ë„ˆëŠ” ë³´í˜¸)"""
        try:
            ecs_client = boto3.client('ecs', region_name='us-east-1')

            # í˜„ì¬ ìš”ì²­ì˜ Task ARN í™•ì¸
            current_task_arn = None
            if self._current_request_id and self._current_request_id in self._sessions:
                session_info = self._sessions[self._current_request_id]
                fargate_session = session_info.get('fargate_session', {})
                current_task_arn = fargate_session.get('task_arn')

            if not current_task_arn:
                logger.warning(f"âš ï¸ No task ARN found for current request {self._current_request_id} - skipping cleanup")
                return

            # âœ… í˜„ì¬ ìš”ì²­ì˜ ì»¨í…Œì´ë„ˆë§Œ ì¢…ë£Œ (ë‹¤ë¥¸ ìš”ì²­ì˜ ì»¨í…Œì´ë„ˆëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ)
            try:
                logger.info(f"ğŸ§¹ Cleaning up orphaned container for request {self._current_request_id}: {current_task_arn.split('/')[-1][:12]}...")
                ecs_client.stop_task(
                    cluster=ECS_CLUSTER_NAME,
                    task=current_task_arn,
                    reason=f'Session creation failed - cleanup (request: {self._current_request_id})'
                )
                logger.info(f"   âœ… Stopped container: {current_task_arn.split('/')[-1][:12]}")
            except Exception as stop_error:
                logger.warning(f"   âš ï¸ Failed to stop container {current_task_arn.split('/')[-1][:12]}: {stop_error}")

        except Exception as e:
            logger.warning(f"âš ï¸ Orphaned container cleanup failed: {e}")

    def _deregister_from_alb(self, container_ip: str):
        """ALB Target Groupì—ì„œ ì»¨í…Œì´ë„ˆ ì œê±°"""
        try:
            elbv2_client = boto3.client('elbv2', region_name=self._session_manager.region)
            elbv2_client.deregister_targets(
                TargetGroupArn=self._session_manager.alb_target_group_arn,
                Targets=[{
                    'Id': container_ip,
                    'Port': 8080
                }]
            )
            logger.info(f"ğŸ”— Deregistered target from ALB: {container_ip}:8080")
        except Exception as alb_error:
            # ALB ì œê±° ì‹¤íŒ¨í•´ë„ ì„¸ì…˜ ì •ë¦¬ëŠ” ê³„ì† ì§„í–‰
            logger.warning(f"âš ï¸ Failed to deregister ALB target {container_ip}: {alb_error}")

    def _auto_cleanup(self):
        """í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ ìë™ìœ¼ë¡œ ëª¨ë“  ì„¸ì…˜ ì •ë¦¬"""
        try:
            if self._sessions:
                logger.info(f"ğŸ§¹ Auto-cleanup: Closing {len(self._sessions)} Fargate sessions on exit...")
                # ëª¨ë“  ì„¸ì…˜ ì •ë¦¬
                for request_id in list(self._sessions.keys()):
                    self.cleanup_session(request_id)

            # âœ… ëª¨ë“  HTTP í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬
            if self._http_clients:
                logger.info(f"ğŸ§¹ Auto-cleanup: Clearing {len(self._http_clients)} HTTP clients...")
                self._http_clients.clear()

            # âœ… ëª¨ë“  ì‹¤íŒ¨ ì¹´ìš´í„° ì •ë¦¬
            if self._session_creation_failures:
                logger.info(f"ğŸ§¹ Auto-cleanup: Clearing {len(self._session_creation_failures)} failure counters...")
                self._session_creation_failures.clear()

            # âœ… ëª¨ë“  cleanup ì¶”ì  ì •ë¦¬
            if self._cleaned_up_requests:
                logger.info(f"ğŸ§¹ Auto-cleanup: Clearing {len(self._cleaned_up_requests)} cleaned-up request trackers...")
                self._cleaned_up_requests.clear()
        except Exception as e:
            logger.warning(f"âš ï¸ Auto-cleanup failed: {e}")

    # ========================================================================
    # âš ï¸ UNUSED FUNCTIONS (COMMENTED OUT FOR REFERENCE)
    # ========================================================================

    # âš ï¸ UNUSED FUNCTION - Commented out (2025-10-12)
    # Not used anywhere in the codebase (only found commented reference in fargate_python_tool.py:120)
    # Kept for potential future reference
    # def get_session_info(self):
    #     """í˜„ì¬ ìš”ì²­ì˜ ì„¸ì…˜ ì •ë³´ ë°˜í™˜"""
    #     if not self._current_request_id:
    #         return {"status": "no_context"}
    #
    #     if self._current_request_id in self._sessions:
    #         session_info = self._sessions[self._current_request_id]
    #         return {
    #             "request_id": self._current_request_id,
    #             "session_id": session_info['session_id'],
    #             "status": "active",
    #             "private_ip": session_info['fargate_session']['private_ip']
    #         }
    #     else:
    #         return {"status": "no_session", "request_id": self._current_request_id}

    # âš ï¸ UNUSED FUNCTION - Commented out (2025-10-12)
    # LEGACY function that creates new timestamp (causes timestamp mismatch)
    # Replaced by _upload_csv_to_s3_with_session_id which uses existing session_id
    # Not used anywhere in the codebase
    # Kept for potential future reference
    # def _upload_csv_to_s3(self, csv_file_path: str) -> str:
    #     """CSV íŒŒì¼ì„ S3ì— ì—…ë¡œë“œ (ë ˆê±°ì‹œ - ìƒˆ timestamp ìƒì„±)"""
    #     try:
    #         # í˜„ì¬ ì„¸ì…˜ ID ê¸°ë°˜ S3 í‚¤ ìƒì„±
    #         timestamp_id = datetime.now().strftime("%Y-%m-%d-%H-%M-%S")
    #
    #         # ì›ë³¸ íŒŒì¼ëª… ì¶”ì¶œ
    #         original_filename = os.path.basename(csv_file_path)
    #         s3_key = f"manus/fargate_sessions/{timestamp_id}/input/{original_filename}"
    #
    #         # S3 ì—…ë¡œë“œ
    #         s3_client = boto3.client('s3', region_name='us-east-1')
    #         s3_client.upload_file(
    #             csv_file_path,
    #             'bedrock-logs-gonsoomoon',
    #             s3_key,
    #             ExtraArgs={'ContentType': 'text/csv'}
    #         )
    #
    #         logger.info(f"ğŸ“¤ Uploaded {csv_file_path} â†’ s3://bedrock-logs-gonsoomoon/{s3_key}")
    #         return s3_key
    #
    #     except Exception as e:
    #         logger.error(f"âŒ S3 upload failed: {e}")
    #         raise


# ============================================================================
# GLOBAL INSTANCE (SINGLETON)
# ============================================================================

# ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
global_fargate_session = GlobalFargateSessionManager()


def get_global_session():
    """ê¸€ë¡œë²Œ ì„¸ì…˜ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return global_fargate_session
