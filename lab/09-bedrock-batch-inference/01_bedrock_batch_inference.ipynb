{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Batchjob 생성 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JONSL 파일 생성 및 S3 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "# S3 클라이언트 생성\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    region_name='us-west-2'\n",
    ")\n",
    "\n",
    "# 버킷 이름\n",
    "bucket_name = 'knowledgebase-bedrock-agent-gsmoon'\n",
    "bucket_prefix = 'batch_inference'\n",
    "\n",
    "# 현재 시간을 이용한 파일 이름 생성\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "file_name = f'batch_records_{timestamp}.jsonl'\n",
    "\n",
    "# 로컬 파일 경로\n",
    "local_file_path = f'/tmp/{file_name}'\n",
    "\n",
    "# JSONL 파일 생성\n",
    "with open(local_file_path, 'w', encoding='utf-8') as file:\n",
    "    for _ in range(100):\n",
    "        # 11자리 영숫자로 된 고유 ID 생성\n",
    "        record_id = uuid.uuid4().hex[:11]\n",
    "        \n",
    "        # 레코드 생성\n",
    "        record = {\n",
    "            \"recordId\": record_id,\n",
    "            \"modelInput\": {\"text\": \"Tell me the fun story of a cat in korean\"}\n",
    "        }\n",
    "        \n",
    "        # JSONL 형식으로 파일에 쓰기\n",
    "        file.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# S3에 파일 업로드\n",
    "try:\n",
    "    s3_client.upload_file(local_file_path, bucket_name, bucket_prefix + '/' + file_name)\n",
    "    print(f\"파일이 성공적으로 업로드되었습니다: s3://{bucket_name}/{bucket_prefix}/{file_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"업로드 중 오류가 발생했습니다: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 정보 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "# SageMaker 세션 생성\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# 기본 IAM 역할 ARN 가져오기\n",
    "role = sagemaker.get_execution_role()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from typing import List, Dict\n",
    "from datetime import datetime as dt\n",
    "\n",
    "def run_batch_inference(\n",
    "    input_s3_uri: str,\n",
    "    output_s3_uri: str,\n",
    "    role_arn: str,\n",
    "    job_name: str,\n",
    "    model_id: str\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Run batch inference using Nova models with batch API\n",
    "    \"\"\"\n",
    "    bedrock = boto3.client('bedrock', region_name='us-west-2')\n",
    "    today_str = dt.today().strftime(\"%y%m%d\")\n",
    "    job_name = f\"nova-pro-batch-{job_name}-{today_str}\"\n",
    "    try:\n",
    "        response = bedrock.create_model_invocation_job(\n",
    "            jobName=job_name,\n",
    "            roleArn=role_arn,\n",
    "            modelId=model_id,\n",
    "            inputDataConfig={\n",
    "                \"s3InputDataConfig\": {\n",
    "                    \"s3Uri\": input_s3_uri,\n",
    "                    \"s3InputFormat\": \"JSONL\"  # JSON에서 JSONL로 변경\n",
    "                }\n",
    "            },\n",
    "            outputDataConfig={\n",
    "                \"s3OutputDataConfig\": {\n",
    "                    \"s3Uri\": output_s3_uri\n",
    "                }\n",
    "            },\n",
    "            timeoutDurationInHours=168\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error during batch inference: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    job_name=f\"job-batch-inference-test-02\"\n",
    "    bucket_output_prefix = \"batch_inference_output\"\n",
    "    input_s3_uri = f\"s3://{bucket_name}/{bucket_prefix}/{file_name}\"\n",
    "    output_s3_uri = f\"s3://{bucket_name}/{bucket_output_prefix}/\"\n",
    "    role_arn = role\n",
    "    # print(f\"job_{num}\")\n",
    "    print(job_name)\n",
    "    print(f\"input_s3_uri: {input_s3_uri}\")\n",
    "    print(f\"output_s3_uri: {output_s3_uri}\")\n",
    "    result = run_batch_inference(\n",
    "        input_s3_uri=input_s3_uri,\n",
    "        output_s3_uri=output_s3_uri,\n",
    "        role_arn=role_arn,\n",
    "        job_name=job_name,\n",
    "        model_id=\"us.amazon.nova-pro-v1:0\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job 생성 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![batch_job_console.png](img/batch_job_console.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
