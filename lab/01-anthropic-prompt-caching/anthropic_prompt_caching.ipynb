{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anthropic Prompt Caching 실험\n",
    "아래의 노트북은 참조 노트북을 \"짧은 한글 문서\" 및 저렴한 Haiku3.0 으로 테스트한 내용 입니다. \n",
    "\n",
    "- 참조: [Prompt caching through the Anthropic API](https://github.com/anthropics/anthropic-cookbook/blob/main/misc/prompt_caching.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt caching through the Anthropic API\n",
    "\n",
    "Prompt caching allows you to store and reuse context within your prompt. This makes it more practical to include additional information in your prompt—such as detailed instructions and example responses—which help improve every response Claude generates.\n",
    "\n",
    "In addition, by fully leveraging prompt caching within your prompt, you can reduce latency by >2x and costs up to 90%. This can generate significant savings when building solutions that involve repetitive tasks around detailed book_content.\n",
    "\n",
    "In this cookbook, we will demonstrate how to use prompt caching in a single turn and across a multi-turn conversation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, let's set up our environment with the necessary imports and initializations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install anthropic bs4 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "client = anthropic.Anthropic(api_key=\"<Type API Key>\")\n",
    "# MODEL_NAME = \"claude-3-5-sonnet-20241022\"\n",
    "MODEL_NAME = \"claude-3-haiku-20240307\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fetch some text content to use in our examples. We'll use the text from Pride and Prejudice by Jane Austen which is around ~187,000 tokens long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_content = \"\"\"\n",
    "태양계는 우주에서 우리의 고향이라 할 수 있는 거대한 천체계입니다. 태양을 중심으로 8개의 행성과 그들의 위성, 소행성, 혜성 등 다양한 천체들이 끊임없이 운동하고 있는 하나의 거대한 시스템입니다. 약 46억 년 전 거대한 성간 물질 구름이 중력에 의해 수축하면서 형성되었으며, 현재까지도 끊임없이 진화하고 있습니다.\n",
    "태양계의 중심인 태양은 전체 질량의 99.86%를 차지하는 거대한 플라즈마 덩어리입니다. 지름이 약 139만 킬로미터에 달하며, 수소와 헬륨의 핵융합 반응을 통해 엄청난 에너지를 방출합니다. 태양의 표면 온도는 약 5,500도이며, 중심부는 1,500만도에 달합니다. 이러한 태양의 강력한 중력이 행성들의 궤도를 유지하게 하는 핵심 요소입니다.\n",
    "태양으로부터 가장 가까운 곳에 위치한 수성은 대기가 거의 없어 극심한 온도 변화를 겪습니다. 낮에는 430도까지 올라가고 밤에는 영하 180도까지 내려가는 극단적인 환경을 가지고 있습니다. 표면은 수많은 운석 충돌 흔적으로 가득하며, 위성은 보유하고 있지 않습니다.\n",
    "금성은 크기와 질량이 지구와 비슷해 '지구의 쌍둥이'라고 불립니다. 하지만 두꺼운 이산화탄소 대기로 인한 극심한 온실효과로 표면 온도가 460도에 달해 태양계에서 가장 뜨거운 행성입니다. 특이하게도 다른 행성들과 반대 방향으로 자전하는 특징이 있습니다.\n",
    "우리의 터전인 지구는 현재까지 발견된 생명체가 존재하는 유일한 행성입니다. 적당한 대기압과 온도, 액체 상태의 물, 자기장의 보호 등 생명체가 살아가기에 완벽한 조건을 갖추고 있습니다. 지구의 유일한 자연위성인 달은 조수현상을 일으키고 지구의 자전축을 안정화시키는 중요한 역할을 합니다.\n",
    "화성은 인류의 다음 탐사 목표로 주목받고 있는 행성입니다. 붉은 색을 띠고 있어 '붉은 행성'으로도 불리며, 과거에 물이 흘렀던 흔적이 발견되어 생명체 존재 가능성에 대한 연구가 활발히 진행되고 있습니다. 두 개의 작은 위성인 포보스와 데이모스를 가지고 있습니다.\n",
    "목성은 태양계 최대의 행성으로, 모든 행성 질량을 합한 것의 2.5배에 달합니다. 수소와 헬륨으로 이루어진 거대한 가스 행성이며, 대적점이라 불리는 거대한 폭풍이 수백 년간 지속되고 있습니다. 현재까지 79개의 위성이 발견되었으며, 희미한 고리도 가지고 있습니다.\n",
    "토성은 아름다운 고리로 유명한 가스 행성입니다. 평균 밀도가 물보다 낮아 충분히 큰 물웅덩이가 있다면 떠오를 수 있을 정도입니다. 82개의 위성을 보유하고 있으며, 그중 타이탄은 두꺼운 대기층을 가진 가장 큰 위성입니다.\n",
    "천왕성은 자전축이 공전면과 거의 수직을 이루고 있어 '누운 행성'으로도 불립니다. 메탄에 의해 청록색을 띠며, 27개의 알려진 위성과 희미한 고리를 가지고 있습니다. 계절의 변화가 매우 극단적으로 나타나는 특징이 있습니다.\n",
    "해왕성은 태양계의 마지막 행성으로, 짙은 청색을 띠며 매우 강한 바람이 부는 것이 특징입니다. 14개의 위성을 가지고 있으며, 가장 큰 위성인 트리톤은 역방향으로 공전하는 특이한 특징을 보입니다.\n",
    "태양계에는 이러한 행성들 외에도 다양한 천체들이 존재합니다. 화성과 목성 사이에 위치한 소행성대에는 수많은 작은 천체들이 분포하고 있으며, 해왕성 궤도 너머의 카이퍼 벨트에는 명왕성을 비롯한 왜소행성들이 있습니다. 가장 외곽에는 오르트 구름이 있어 많은 혜성의 기원이 되고 있습니다.\n",
    "인류는 다양한 우주 탐사선을 통해 태양계를 연구하고 있습니다. 보이저호, 카시니호, 뉴호라이즌스 등의 탐사선들이 새로운 발견을 이어가고 있으며, 특히 화성 탐사와 생명체 탐색에 많은 노력을 기울이고 있습니다.\n",
    "미래에는 화성 정착, 소행성 자원 채굴, 새로운 거주지 탐색 등이 주요 과제가 될 것으로 예상됩니다. 약 50억 년 후에는 태양이 적색거성으로 진화하면서 태양계의 구조가 크게 변화할 것으로 예측되고 있습니다.\n",
    "이처럼 태양계는 끊임없이 변화하고 진화하는 역동적인 시스템이며, 인류에게 무한한 탐구의 대상이 되고 있습니다. 우주 기술의 발전과 함께 태양계에 대한 이해도 계속해서 깊어지고 있으며, 앞으로도 많은 새로운 발견이 이어질 것으로 기대됩니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n태양계는 우주에서 우리의 고향이라 할 수 있는 거대한 천체계입니다. 태양을 중심으로 8개의 행성과 그들의 위성, 소행성, 혜성 등 다양한 천체들이 끊임없이 운동하고 있는 하나의 거대한 시스템입니다. 약 46억 년 전 거대한 성간 물질 구름이 중력에 의해 수축하면서 형성되었으며, 현재까지도 끊임없이 진화하고 있습니다.\\n태양계의 중심인 태양은 전체 질량의 99.86%를 차지하는 거대한 플라즈마 덩어리입니다. 지름이 약 139만 킬로미터에 달하며, 수소와 헬륨의 핵융합 반응을 통해 엄청난 에너지를 방출합니다. 태양의 표면 온도는 약 5,500도이며, 중심부는 1,500만도에 달합니다. 이러한 태양의 강력한 중력이 행성들의 궤도를 유지하게 하는 핵심 요소입니다.\\n태양으로부터 가장 가까운 곳에 위치한 수성은 대기가 거의 없어 극심한 온도 변화를 겪습니다. 낮에는 430도까지 올라가고 밤에는 영하 180도까지 내려가는 극단적인 환경을 가지고 있습니다. 표면은 수많은 운석 충돌 흔적으로 가득하며, 위성은 보유하고 있지 않습니다.\\n금성은 크기와 질량이 지구와 비슷해 '지구의 쌍둥이'라고 불립니다. 하지만 두꺼운 이산화탄소 대기로 인한 극심한 온실효과로 표면 온도가 460도에 달해 태양계에서 가장 뜨거운 행성입니다. 특이하게도 다른 행성들과 반대 방향으로 자전하는 특징이 있습니다.\\n우리의 터전인 지구는 현재까지 발견된 생명체가 존재하는 유일한 행성입니다. 적당한 대기압과 온도, 액체 상태의 물, 자기장의 보호 등 생명체가 살아가기에 완벽한 조건을 갖추고 있습니다. 지구의 유일한 자연위성인 달은 조수현상을 일으키고 지구의 자전축을 안정화시키는 중요한 역할을 합니다.\\n화성은 인류의 다음 탐사 목표로 주목받고 있는 행성입니다. 붉은 색을 띠고 있어 '붉은 행성'으로도 불리며, 과거에 물이 흘렀던 흔적이 발견되어 생명체 존재 가능성에 대한 연구가 활발히 진행되고 있습니다. 두 개의 작은 위성인 포보스와 데이모스를 가지고 있습니다.\\n목성은 태양계 최대의 행성으로, 모든 행성 질량을 합한 것의 2.5배에 달합니다. 수소와 헬륨으로 이루어진 거대한 가스 행성이며, 대적점이라 불리는 거대한 폭풍이 수백 년간 지속되고 있습니다. 현재까지 79개의 위성이 발견되었으며, 희미한 고리도 가지고 있습니다.\\n토성은 아름다운 고리로 유명한 가스 행성입니다. 평균 밀도가 물보다 낮아 충분히 큰 물웅덩이가 있다면 떠오를 수 있을 정도입니다. 82개의 위성을 보유하고 있으며, 그중 타이탄은 두꺼운 대기층을 가진 가장 큰 위성입니다.\\n천왕성은 자전축이 공전면과 거의 수직을 이루고 있어 '누운 행성'으로도 불립니다. 메탄에 의해 청록색을 띠며, 27개의 알려진 위성과 희미한 고리를 가지고 있습니다. 계절의 변화가 매우 극단적으로 나타나는 특징이 있습니다.\\n해왕성은 태양계의 마지막 행성으로, 짙은 청색을 띠며 매우 강한 바람이 부는 것이 특징입니다. 14개의 위성을 가지고 있으며, 가장 큰 위성인 트리톤은 역방향으로 공전하는 특이한 특징을 보입니다.\\n태양계에는 이러한 행성들 외에도 다양한 천체들이 존재합니다. 화성과 목성 사이에 위치한 소행성대에는 수많은 작은 천체들이 분포하고 있으며, 해왕성 궤도 너머의 카이퍼 벨트에는 명왕성을 비롯한 왜소행성들이 있습니다. 가장 외곽에는 오르트 구름이 있어 많은 혜성의 기원이 되고 있습니다.\\n인류는 다양한 우주 탐사선을 통해 태양계를 연구하고 있습니다. 보이저호, 카시니호, 뉴호라이즌스 등의 탐사선들이 새로운 발견을 이어가고 있으며, 특히 화성 탐사와 생명체 탐색에 많은 노력을 기울이고 있습니다.\\n미래에는 화성 정착, 소행성 자원 채굴, 새로운 거주지 탐색 등이 주요 과제가 될 것으로 예상됩니다. 약 50억 년 후에는 태양이 적색거성으로 진화하면서 태양계의 구조가 크게 변화할 것으로 예측되고 있습니다.\\n이처럼 태양계는 끊임없이 변화하고 진화하는 역동적인 시스템이며, 인류에게 무한한 탐구의 대상이 되고 있습니다. 우주 기술의 발전과 함께 태양계에 대한 이해도 계속해서 깊어지고 있으며, 앞으로도 많은 새로운 발견이 이어질 것으로 기대됩니다.\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Single turn\n",
    "\n",
    "Let's demonstrate prompt caching with a large document, comparing the performance and cost between cached and non-cached API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Non-cached API Call\n",
    "\n",
    "First, let's make a non-cached API call. This will load the prompt into the cache so that our subsequent cached API calls can benefit from the prompt caching.\n",
    "\n",
    "We will ask for a short output string to keep the output response time low since the benefit of prompt caching applies only to the input processing time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 메세지 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_content_val:  2019\n"
     ]
    }
   ],
   "source": [
    "def get_message():\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<book>\" + book_content + \"</book>\",\n",
    "                    \"cache_control\": {\"type\": \"ephemeral\"}\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"다른 행성들과 반대 방향으로 자전하는 행성은 뭐야? 답변은 단답식으로 말해줘. (예: 수성)\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "\n",
    "book_content_val = get_message()\n",
    "book_content_val = book_content_val[0]['content'][0]['text']\n",
    "print(\"book_content_val: \", len(book_content_val))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Call 을 하면서 캐시 Write 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-cached API call time: 0.48 seconds\n",
      "Non-cached API call input tokens: 57\n",
      "Non-cached API call output tokens: 10\n",
      "Non-cached API call cache_creation_input_tokens: 2079\n",
      "Non-cached API call cache_read_input_tokens: 0\n",
      "\n",
      "Summary (non-cached):\n",
      "[TextBlock(text='금성입니다.', type='text')]\n"
     ]
    }
   ],
   "source": [
    "def make_non_cached_api_call():\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<book>\" + book_content + \"</book>\",\n",
    "                    \"cache_control\": {\"type\": \"ephemeral\"}\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"다른 행성들과 반대 방향으로 자전하는 행성은 뭐야? 답변은 단답식으로 말해줘. (예: 수성)\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = client.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=300,\n",
    "        messages=messages,\n",
    "        extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"}\n",
    "\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    return response, end_time - start_time\n",
    "\n",
    "non_cached_response, non_cached_time = make_non_cached_api_call()\n",
    "\n",
    "print(f\"Non-cached API call time: {non_cached_time:.2f} seconds\")\n",
    "print(f\"Non-cached API call input tokens: {non_cached_response.usage.input_tokens}\")\n",
    "print(f\"Non-cached API call output tokens: {non_cached_response.usage.output_tokens}\")\n",
    "print(f\"Non-cached API call cache_creation_input_tokens: {non_cached_response.usage.cache_creation_input_tokens}\")\n",
    "print(f\"Non-cached API call cache_read_input_tokens: {non_cached_response.usage.cache_read_input_tokens}\")\n",
    "\n",
    "\n",
    "print(\"\\nSummary (non-cached):\")\n",
    "print(non_cached_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='msg_01EtsRGUr2Hj31PJ9pPJkny3', content=[TextBlock(text='금성입니다.', type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=57, output_tokens=10, cache_creation_input_tokens=2079, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_cached_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Cached API Call\n",
    "\n",
    "Now, let's make a cached API call. I'll add in the \"cache_control\": {\"type\": \"ephemeral\"} attribute to the content object and add the \"prompt-caching-2024-07-31\" beta header to the request. This will enable prompt caching for this API call.\n",
    "\n",
    "To keep the output latency constant, we will ask Claude the same question as before. Note that this question is not part of the cached content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached API call time: 0.54 seconds\n",
      "Cached API call input tokens: 57\n",
      "Cached API call output tokens: 6\n",
      "Non-cached API call cache_creation_input_tokens: 0\n",
      "Non-cached API call cache_read_input_tokens: 2079\n",
      "\n",
      "Summary (cached):\n",
      "[TextBlock(text='금성', type='text')]\n"
     ]
    }
   ],
   "source": [
    "def make_cached_api_call():\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<book>\" + book_content + \"</book>\",\n",
    "                    \"cache_control\": {\"type\": \"ephemeral\"}\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"다른 행성들과 반대 방향으로 자전하는 행성은 뭐야? 답변은 단답식으로 말해줘. (예: 수성)\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = client.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=300,\n",
    "        messages=messages,\n",
    "        extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"}\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    return response, end_time - start_time\n",
    "\n",
    "cached_response, cached_time = make_cached_api_call()\n",
    "\n",
    "print(f\"Cached API call time: {cached_time:.2f} seconds\")\n",
    "print(f\"Cached API call input tokens: {cached_response.usage.input_tokens}\")\n",
    "print(f\"Cached API call output tokens: {cached_response.usage.output_tokens}\")\n",
    "print(f\"Non-cached API call cache_creation_input_tokens: {cached_response.usage.cache_creation_input_tokens}\")\n",
    "print(f\"Non-cached API call cache_read_input_tokens: {cached_response.usage.cache_read_input_tokens}\")\n",
    "\n",
    "print(\"\\nSummary (cached):\")\n",
    "print(cached_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='msg_01JG6yoCafqJpV8DnapHDWjy', content=[TextBlock(text='금성', type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=57, output_tokens=6, cache_creation_input_tokens=0, cache_read_input_tokens=2079))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the cached API call only took 3.64 seconds total compared to 21.44 seconds for the non-cached API call. This is a significant improvement in overall latency due to caching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Multi-turn Conversation with Incremental Caching\n",
    "\n",
    "Now, let's look at a multi-turn conversation where we add cache breakpoints as the conversation progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Turn 1:\n",
      "User: 다른 행성들과 반대 방향으로 자전하는 행성은 뭐야? 답변은 단답식으로 말해줘. (예: 수성)\n",
      "## messages: \n",
      " [{'role': 'user', 'content': [{'type': 'text', 'text': '다른 행성들과 반대 방향으로 자전하는 행성은 뭐야? 답변은 단답식으로 말해줘. (예: 수성)', 'cache_control': {'type': 'ephemeral'}}]}]\n",
      "Assistant: 금성\n",
      "User input tokens: 4\n",
      "Output tokens: 6\n",
      "Input tokens (cache read): 0\n",
      "Input tokens (cache write): 2139\n",
      "0.0% of input prompt cached (4 tokens)\n",
      "Time taken: 0.46 seconds\n",
      "\n",
      "Turn 2:\n",
      "User: 붉은 색을 띠고 있어 붉은 행성 으로도 불리며, 과거에 물이 흘렀던 흔적이 발견되어 생명체 존재 가능성이 있는 행성은 뭐야? 답변은 단답식으로 말해줘. (예: 수성)\n",
      "## messages: \n",
      " [{'role': 'user', 'content': [{'type': 'text', 'text': '다른 행성들과 반대 방향으로 자전하는 행성은 뭐야? 답변은 단답식으로 말해줘. (예: 수성)', 'cache_control': {'type': 'ephemeral'}}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '금성'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': '붉은 색을 띠고 있어 붉은 행성 으로도 불리며, 과거에 물이 흘렀던 흔적이 발견되어 생명체 존재 가능성이 있는 행성은 뭐야? 답변은 단답식으로 말해줘. (예: 수성)', 'cache_control': {'type': 'ephemeral'}}]}]\n",
      "Assistant: 화성\n",
      "User input tokens: 4\n",
      "Output tokens: 6\n",
      "Input tokens (cache read): 2139\n",
      "Input tokens (cache write): 110\n",
      "99.8% of input prompt cached (2143 tokens)\n",
      "Time taken: 0.48 seconds\n",
      "\n",
      "Turn 3:\n",
      "User: 82개의 위성을 보유하고 있으며, 그중 타이탄은 두꺼운 대기층을 가진 가장 큰 위성을 가진 행성은 뭐야? 답변은 단답식으로 말해줘. (예: 수성)\n",
      "## messages: \n",
      " [{'role': 'user', 'content': [{'type': 'text', 'text': '다른 행성들과 반대 방향으로 자전하는 행성은 뭐야? 답변은 단답식으로 말해줘. (예: 수성)'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '금성'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': '붉은 색을 띠고 있어 붉은 행성 으로도 불리며, 과거에 물이 흘렀던 흔적이 발견되어 생명체 존재 가능성이 있는 행성은 뭐야? 답변은 단답식으로 말해줘. (예: 수성)', 'cache_control': {'type': 'ephemeral'}}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '화성'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': '82개의 위성을 보유하고 있으며, 그중 타이탄은 두꺼운 대기층을 가진 가장 큰 위성을 가진 행성은 뭐야? 답변은 단답식으로 말해줘. (예: 수성)', 'cache_control': {'type': 'ephemeral'}}]}]\n",
      "Assistant: 토성\n",
      "User input tokens: 4\n",
      "Output tokens: 6\n",
      "Input tokens (cache read): 2249\n",
      "Input tokens (cache write): 96\n",
      "99.8% of input prompt cached (2253 tokens)\n",
      "Time taken: 0.70 seconds\n",
      "\n",
      "Turn 4:\n",
      "User: 짙은 청색을 띠며 매우 강한 바람이 부는 것이 특징인 행성은 뭐야? 답변은 단답식으로 말해줘. (예: 수성)\n",
      "## messages: \n",
      " [{'role': 'user', 'content': [{'type': 'text', 'text': '다른 행성들과 반대 방향으로 자전하는 행성은 뭐야? 답변은 단답식으로 말해줘. (예: 수성)'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '금성'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': '붉은 색을 띠고 있어 붉은 행성 으로도 불리며, 과거에 물이 흘렀던 흔적이 발견되어 생명체 존재 가능성이 있는 행성은 뭐야? 답변은 단답식으로 말해줘. (예: 수성)'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '화성'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': '82개의 위성을 보유하고 있으며, 그중 타이탄은 두꺼운 대기층을 가진 가장 큰 위성을 가진 행성은 뭐야? 답변은 단답식으로 말해줘. (예: 수성)', 'cache_control': {'type': 'ephemeral'}}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '토성'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': '짙은 청색을 띠며 매우 강한 바람이 부는 것이 특징인 행성은 뭐야? 답변은 단답식으로 말해줘. (예: 수성)', 'cache_control': {'type': 'ephemeral'}}]}]\n",
      "Assistant: 해왕성\n",
      "User input tokens: 4\n",
      "Output tokens: 8\n",
      "Input tokens (cache read): 2345\n",
      "Input tokens (cache write): 74\n",
      "99.8% of input prompt cached (2349 tokens)\n",
      "Time taken: 0.50 seconds\n"
     ]
    }
   ],
   "source": [
    "class ConversationHistory:\n",
    "    def __init__(self):\n",
    "        # Initialize an empty list to store conversation turns\n",
    "        self.turns = []\n",
    "\n",
    "    def add_turn_assistant(self, content):\n",
    "        # Add an assistant's turn to the conversation history\n",
    "        self.turns.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": content\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    def add_turn_user(self, content):\n",
    "        # Add a user's turn to the conversation history\n",
    "        self.turns.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": content\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    def get_turns(self):\n",
    "        # Retrieve conversation turns with specific formatting\n",
    "        result = []\n",
    "        user_turns_processed = 0\n",
    "        # Iterate through turns in reverse order\n",
    "        for turn in reversed(self.turns):\n",
    "            if turn[\"role\"] == \"user\" and user_turns_processed < 2:\n",
    "                # Add the last two user turns with ephemeral cache control\n",
    "                result.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": turn[\"content\"][0][\"text\"],\n",
    "                            \"cache_control\": {\"type\": \"ephemeral\"}\n",
    "                        }\n",
    "                    ]\n",
    "                })\n",
    "                user_turns_processed += 1\n",
    "            else:\n",
    "                # Add other turns as they are\n",
    "                result.append(turn)\n",
    "        # Return the turns in the original order\n",
    "        return list(reversed(result))\n",
    "\n",
    "# Initialize the conversation history\n",
    "conversation_history = ConversationHistory()\n",
    "\n",
    "# System message containing the book content\n",
    "# Note: 'book_content' should be defined elsewhere in the code\n",
    "system_message = f\"<file_contents> {book_content} </file_contents>\"\n",
    "\n",
    "# Predefined questions for our simulation\n",
    "questions = [\n",
    "    \"다른 행성들과 반대 방향으로 자전하는 행성은 뭐야? 답변은 단답식으로 말해줘. (예: 수성)\",\n",
    "    \"붉은 색을 띠고 있어 붉은 행성 으로도 불리며, 과거에 물이 흘렀던 흔적이 발견되어 생명체 존재 가능성이 있는 행성은 뭐야? 답변은 단답식으로 말해줘. (예: 수성)\",\n",
    "    \"82개의 위성을 보유하고 있으며, 그중 타이탄은 두꺼운 대기층을 가진 가장 큰 위성을 가진 행성은 뭐야? 답변은 단답식으로 말해줘. (예: 수성)\",\n",
    "    \"짙은 청색을 띠며 매우 강한 바람이 부는 것이 특징인 행성은 뭐야? 답변은 단답식으로 말해줘. (예: 수성)\"\n",
    "]\n",
    "\n",
    "def simulate_conversation():\n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"\\nTurn {i}:\")\n",
    "        print(f\"User: {question}\")\n",
    "        \n",
    "        # Add user input to conversation history\n",
    "        conversation_history.add_turn_user(question)\n",
    "        messages=conversation_history.get_turns()\n",
    "        print(\"## messages: \\n\", messages)\n",
    "\n",
    "        # Record the start time for performance measurement\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Make an API call to the assistant\n",
    "        response = client.messages.create(\n",
    "            model=MODEL_NAME,\n",
    "            extra_headers={\n",
    "              \"anthropic-beta\": \"prompt-caching-2024-07-31\"\n",
    "            },\n",
    "            max_tokens=300,\n",
    "            system=[\n",
    "                {\"type\": \"text\", \"text\": system_message, \"cache_control\": {\"type\": \"ephemeral\"}},\n",
    "            ],\n",
    "            messages=messages,\n",
    "        )\n",
    "\n",
    "        # Record the end time\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Extract the assistant's reply\n",
    "        assistant_reply = response.content[0].text\n",
    "        print(f\"Assistant: {assistant_reply}\")\n",
    "\n",
    "        # Print token usage information\n",
    "        input_tokens = response.usage.input_tokens\n",
    "        output_tokens = response.usage.output_tokens\n",
    "        input_tokens_cache_read = getattr(response.usage, 'cache_read_input_tokens', '---')\n",
    "        input_tokens_cache_create = getattr(response.usage, 'cache_creation_input_tokens', '---')\n",
    "        print(f\"User input tokens: {input_tokens}\")\n",
    "        print(f\"Output tokens: {output_tokens}\")\n",
    "        print(f\"Input tokens (cache read): {input_tokens_cache_read}\")\n",
    "        print(f\"Input tokens (cache write): {input_tokens_cache_create}\")\n",
    "\n",
    "        # Calculate and print the elapsed time\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        # Calculate the percentage of input prompt cached\n",
    "        total_input_tokens = input_tokens + (int(input_tokens_cache_read) if input_tokens_cache_read != '---' else 0)\n",
    "        percentage_cached = (int(input_tokens_cache_read) / total_input_tokens * 100 if input_tokens_cache_read != '---' and total_input_tokens > 0 else 0)\n",
    "\n",
    "        print(f\"{percentage_cached:.1f}% of input prompt cached ({total_input_tokens} tokens)\")\n",
    "        print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "        # Add assistant's reply to conversation history\n",
    "        conversation_history.add_turn_assistant(assistant_reply)\n",
    "\n",
    "# Run the simulated conversation\n",
    "simulate_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in this example, response times decreased from nearly 24 seconds to just 7-11 seconds after the initial cache setup, while maintaining the same level of quality across the answers. Most of this remaining latency is due to the time it takes to generate the response, which is not affected by prompt caching.\n",
    "\n",
    "And since nearly 100% of input tokens were cached in subsequent turns as we kept adjusting the cache breakpoints, we were able to read the next user message nearly instantly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
